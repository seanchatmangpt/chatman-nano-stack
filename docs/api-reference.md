# API Reference

Complete API documentation for CNS Ontology Forge components.

## 🎯 Overview

CNS Ontology Forge provides multiple APIs for different aspects of ontology engineering:

- **[Meta Layer API](#meta-layer-api)** - Automated ontology generation
- **[Quality Control API](#quality-control-api)** - AI-powered validation
- **[TTL2DSPy API](#ttl2dspy-api)** - Transpiler functionality
- **[Performance API](#performance-api)** - Benchmarking and monitoring
- **[CLI API](#cli-api)** - Command-line interfaces

## 📦 Meta Layer API

### OntologyMetaForge

Main class for automated ontology generation.

```python
from ontology_meta_forge import OntologyMetaForge, DomainType, MetaOntologySpec

forge = OntologyMetaForge()
```

#### Methods

##### `generate_meta_spec(domain: str, requirements: str, custom_modules: Optional[List[str]] = None) -> MetaOntologySpec`

Generate meta specification from high-level requirements.

**Parameters:**
- `domain` (str): Domain type ('trading', 'healthcare', 'iot', 'automotive', 'custom')
- `requirements` (str): Natural language requirements description
- `custom_modules` (Optional[List[str]]): Additional modules to include

**Returns:**
- `MetaOntologySpec`: Complete specification for ontology generation

**Example:**
```python
spec = forge.generate_meta_spec(
    domain="trading",
    requirements="Ultra-high-frequency trading with 8-tick compliance",
    custom_modules=["compliance", "monitoring"]
)
```

##### `forge_domain_ontologies(meta_spec: MetaOntologySpec, output_dir: Path) -> Dict[str, Path]`

Generate complete ontology suite from meta specification.

**Parameters:**
- `meta_spec` (MetaOntologySpec): Specification generated by `generate_meta_spec`
- `output_dir` (Path): Output directory for generated files

**Returns:**
- `Dict[str, Path]`: Mapping of module names to generated file paths

**Example:**
```python
generated_files = forge.forge_domain_ontologies(spec, Path("ontologies/output"))
# Returns: {"core": Path("ontologies/output/core.ttl"), ...}
```

### MetaOntologySpec

Data class representing ontology specification.

```python
@dataclass
class MetaOntologySpec:
    domain: DomainType
    name: str
    description: str
    modules: List[str]
    performance_requirements: Dict[str, Any]
    compliance_standards: List[str] = field(default_factory=list)
    custom_patterns: List[DomainPattern] = field(default_factory=list)
```

**Attributes:**
- `domain` (DomainType): Target domain enumeration
- `name` (str): Human-readable ontology name
- `description` (str): Detailed description
- `modules` (List[str]): List of modules to generate
- `performance_requirements` (Dict[str, Any]): Performance constraints
- `compliance_standards` (List[str]): Required compliance standards

### DomainPattern

Reusable pattern for domain ontologies.

```python
@dataclass
class DomainPattern:
    name: str
    core_classes: List[str]
    properties: List[str]
    constraints: List[str]
    performance_targets: Dict[str, int]
```

**Example:**
```python
trading_pattern = DomainPattern(
    name="Ultra-High-Frequency Trading",
    core_classes=["Order", "OrderBook", "MatchingEngine"],
    properties=["price", "quantity", "timestamp"],
    constraints=["8tick_compliance", "risk_limits"],
    performance_targets={"order_match": 8, "risk_check": 8}
)
```

## 🔍 Quality Control API

### OntologyQualityController

Main class for AI-powered quality validation.

```python
from ontology_quality_control import OntologyQualityController, QualityReport

qc = OntologyQualityController(model="qwen3:latest")
```

#### Methods

##### `validate_ontology_suite(ontology_dir: Path, domain: str) -> QualityReport`

Comprehensive validation of ontology suite.

**Parameters:**
- `ontology_dir` (Path): Directory containing ontology files
- `domain` (str): Domain context for validation

**Returns:**
- `QualityReport`: Detailed validation results

**Example:**
```python
report = qc.validate_ontology_suite(
    Path("ontologies/trading"), 
    "trading"
)

print(f"Quality Score: {report.metrics['quality_score']:.1f}/100")
print(f"Critical Issues: {report.critical_count}")
print(f"Passed: {report.passed}")
```

##### `optimize_ontology(file_path: Path, domain: str, performance_reqs: Dict[str, Any]) -> Tuple[str, List[str]]`

Optimize ontology using DSPy reasoning.

**Parameters:**
- `file_path` (Path): Path to TTL file to optimize
- `domain` (str): Domain context
- `performance_reqs` (Dict[str, Any]): Performance requirements

**Returns:**
- `Tuple[str, List[str]]`: (optimized_content, list_of_optimizations)

##### `auto_fix_issues(ontology_dir: Path, issues: List[QualityIssue]) -> Dict[str, List[str]]`

Automatically fix resolvable issues.

**Parameters:**
- `ontology_dir` (Path): Directory with ontology files
- `issues` (List[QualityIssue]): Issues to fix

**Returns:**
- `Dict[str, List[str]]`: Mapping of file paths to applied fixes

### QualityReport

Data class containing validation results.

```python
@dataclass
class QualityReport:
    domain: str
    timestamp: str
    total_files: int
    issues: List[QualityIssue] = field(default_factory=list)
    metrics: Dict[str, Any] = field(default_factory=dict)
    passed: bool = False
    
    @property
    def critical_count(self) -> int
    
    @property  
    def warning_count(self) -> int
```

**Key Properties:**
- `critical_count`: Number of critical issues (blocks deployment)
- `warning_count`: Number of warning-level issues
- `passed`: Boolean indicating if validation passed
- `metrics`: Detailed quality metrics dictionary

### QualityIssue

Individual quality issue detected during validation.

```python
@dataclass
class QualityIssue:
    rule: ValidationRule
    level: QualityLevel
    message: str
    file_path: Optional[str] = None
    line_number: Optional[int] = None
    suggestion: Optional[str] = None
    auto_fixable: bool = False
```

**Enums:**
```python
class QualityLevel(Enum):
    CRITICAL = "critical"
    WARNING = "warning"
    INFO = "info"

class ValidationRule(Enum):
    SEMANTIC_CONSISTENCY = "semantic_consistency"
    PERFORMANCE_COMPLIANCE = "performance_compliance"
    STANDARD_COMPLIANCE = "standard_compliance"
    SCHEMA_VALIDATION = "schema_validation"
    # ... more rules
```

## 🤖 Multi-Agent API

### OntologyAgentSwarm

Multi-agent system for sophisticated ontology analysis.

```python
from dspy_ontology_agents import OntologyAgentSwarm, AgentRole

swarm = OntologyAgentSwarm(model="qwen3:latest")
```

#### Methods

##### `analyze_ontology_suite(ontology_dir: Path, domain: str, requirements: Dict[str, Any]) -> Dict[str, Any]`

Run multi-agent analysis of ontology suite.

**Parameters:**
- `ontology_dir` (Path): Directory with ontology files
- `domain` (str): Domain context
- `requirements` (Dict[str, Any]): Analysis requirements

**Returns:**
- `Dict[str, Any]`: Comprehensive analysis results from all agents

**Example:**
```python
requirements = {
    "performance_requirements": {"tick_compliance": 8},
    "compliance_standards": ["MiFID_II"],
    "hardware_constraints": {"target_latency_ns": 100}
}

analysis = swarm.analyze_ontology_suite(
    Path("ontologies/trading"),
    "trading", 
    requirements
)

# Access agent results
domain_score = analysis["domain_analysis"]["accuracy_score"]
performance_score = analysis["performance_analysis"]["performance_score"]
overall_quality = analysis["quality_synthesis"]["overall_quality_score"]
```

##### `generate_agent_collaboration_report() -> str`

Generate markdown report of agent collaboration.

**Returns:**
- `str`: Markdown-formatted collaboration report

### CollaborativeOntologyOptimizer

Multi-agent collaborative optimizer.

```python
from dspy_ontology_agents import CollaborativeOntologyOptimizer

optimizer = CollaborativeOntologyOptimizer(model="qwen3:latest")
```

#### Methods

##### `iterative_optimization(ontology_dir: Path, domain: str, requirements: Dict[str, Any], max_iterations: int = 3) -> Dict[str, Any]`

Iteratively optimize ontology using multi-agent feedback.

**Parameters:**
- `ontology_dir` (Path): Directory with ontology files
- `domain` (str): Domain context
- `requirements` (Dict[str, Any]): Optimization requirements
- `max_iterations` (int): Maximum optimization rounds

**Returns:**
- `Dict[str, Any]`: Optimization history and results

## 🔧 TTL2DSPy API

### TTL2DSPyTranspiler

Transpiler for converting TTL ontologies to DSPy signatures.

```python
from ttl2dspy import TTL2DSPyTranspiler

transpiler = TTL2DSPyTranspiler()
```

#### Methods

##### `build_signatures(g: Graph) -> Dict[str, str]`

Build DSPy signatures from RDF graph.

**Parameters:**
- `g` (Graph): RDFLib graph with ontology and SHACL shapes

**Returns:**
- `Dict[str, str]`: Mapping of signature names to generated code

##### `generate_module(signatures: Dict[str, str], ontology_uri: str = "") -> str`

Generate complete Python module with all signatures.

**Parameters:**
- `signatures` (Dict[str, str]): Generated signature code
- `ontology_uri` (str): Source ontology URI

**Returns:**
- `str`: Complete Python module code

### Helper Functions

##### `parse_ontology(ttl_file: Path) -> Tuple[Graph, str]`

Parse TTL ontology file.

**Parameters:**
- `ttl_file` (Path): Path to TTL file

**Returns:**
- `Tuple[Graph, str]`: (RDFLib graph, ontology URI)

##### `write_signature_file(signatures: Dict[str, str], output_file: Path, ontology_uri: str, merge_mode: bool = False) -> bool`

Write signatures to Python file.

**Parameters:**
- `signatures` (Dict[str, str]): Generated signatures
- `output_file` (Path): Output file path
- `ontology_uri` (str): Source ontology URI
- `merge_mode` (bool): Whether to generate merged module

**Returns:**
- `bool`: Success/failure status

## 📊 Performance API

### Benchmark Functions

```python
from src.benchmark.otel_benchmark import otel_context_t, otel_init, otel_start_timing, otel_end_timing, otel_report_mermaid
```

#### Core Functions

##### `otel_init(ctx: otel_context_t) -> None`

Initialize OpenTelemetry context.

##### `otel_start_timing(ctx: otel_context_t, name: str) -> None`

Start timing measurement.

##### `otel_end_timing(ctx: otel_context_t, name: str, count: int) -> None`

End timing measurement.

##### `otel_report_mermaid(ctx: otel_context_t) -> None`

Generate Mermaid diagram report.

### Performance Monitoring

```python
def monitor_performance(binary_path: str, duration: int = 60) -> PerformanceMetrics
def analyze_latency_distribution(latencies: List[float]) -> Dict[str, float]
def check_8tick_compliance(latencies: List[float]) -> bool
```

## ⌨️ CLI API

### Meta Forge CLI

```bash
# Generate ontology from requirements
python ontology_meta_forge.py <domain> "<requirements>"

# Advanced orchestration
python meta_forge_orchestrator.py generate <domain> "<requirements>"
python meta_forge_orchestrator.py batch-generate <config.yaml>
python meta_forge_orchestrator.py analyze <ontology_dir>
```

### Advanced Forge CLI

```bash
# Comprehensive validation
python advanced_forge_cli.py validate <ontology_dir> [options]

# AI-powered optimization  
python advanced_forge_cli.py optimize <ontology_dir> [options]

# Compliance auditing
python advanced_forge_cli.py audit <ontology_dir> [options]

# Performance benchmarking
python advanced_forge_cli.py benchmark <ontology_dir> [options]

# AI explanation
python advanced_forge_cli.py explain <ontology_file> [options]

# Multi-dimensional comparison
python advanced_forge_cli.py compare <dir1> <dir2> [options]

# Quality-gated deployment
python advanced_forge_cli.py deploy <ontology_dir> [options]
```

### TTL2DSPy CLI

```bash
# Basic conversion
python ttl2dspy.py <input.ttl> <output.py>

# Batch processing
python ttl2dspy.py <input_pattern> <output_dir> --batch

# Merge mode
python ttl2dspy.py <input_pattern> <output.py> --merge

# Verbose output
python ttl2dspy.py <input.ttl> <output.py> --verbose
```

## 🎯 Usage Examples

### Complete Workflow

```python
from pathlib import Path
from ontology_meta_forge import OntologyMetaForge
from ontology_quality_control import OntologyQualityController
from dspy_ontology_agents import OntologyAgentSwarm
from ttl2dspy import TTL2DSPyTranspiler, parse_ontology

# Step 1: Generate ontology
forge = OntologyMetaForge()
spec = forge.generate_meta_spec("trading", "8-tick UHFT system")
ontologies = forge.forge_domain_ontologies(spec, Path("output"))

# Step 2: Quality validation
qc = OntologyQualityController()
report = qc.validate_ontology_suite(Path("output"), "trading")

if not report.passed:
    # Auto-fix issues
    fixes = qc.auto_fix_issues(Path("output"), report.issues)
    report = qc.validate_ontology_suite(Path("output"), "trading")

# Step 3: AI analysis
swarm = OntologyAgentSwarm()
analysis = swarm.analyze_ontology_suite(
    Path("output"), "trading", 
    {"performance_requirements": {"tick_compliance": 8}}
)

# Step 4: Convert to DSPy
transpiler = TTL2DSPyTranspiler()
for module, ttl_path in ontologies.items():
    g, uri = parse_ontology(ttl_path)
    signatures = transpiler.build_signatures(g)
    
    output_file = Path(f"{module}_signatures.py")
    module_code = transpiler.generate_module(signatures, uri)
    output_file.write_text(module_code)

print("✅ Complete workflow finished successfully!")
```

### Error Handling

```python
try:
    report = qc.validate_ontology_suite(ontology_dir, domain)
except Exception as e:
    print(f"Validation error: {e}")
    # Handle validation failure
    
if report.critical_count > 0:
    print(f"Critical issues found: {report.critical_count}")
    for issue in report.issues:
        if issue.level == QualityLevel.CRITICAL:
            print(f"  - {issue.message}")
            if issue.auto_fixable:
                print(f"    💡 {issue.suggestion}")
```

### Custom Configuration

```python
# Custom quality controller
qc = OntologyQualityController(
    model="qwen3:latest",
    validation_rules={
        ValidationRule.PERFORMANCE_COMPLIANCE: {
            "tick_limit": 4,  # Stricter than default 8
            "memory_threshold": "512KB"
        }
    }
)

# Custom agent swarm
swarm = OntologyAgentSwarm(
    model="qwen3:latest",
    agent_config={
        "domain_expert": {"expertise_level": "senior"},
        "performance_engineer": {"optimization_focus": "latency"}
    }
)
```

## 🔧 Configuration

### Environment Variables

```bash
# Ollama configuration
export OLLAMA_HOST="http://localhost:11434"
export OLLAMA_MODEL="qwen3:latest"

# Performance settings
export CNS_TICK_LIMIT=8
export CNS_MEMORY_THRESHOLD="1MB"
export CNS_CACHE_SIZE="64MB"

# Quality gates
export CNS_MIN_QUALITY_SCORE=95
export CNS_MAX_CRITICAL_ISSUES=0
```

### Configuration Files

#### `meta_forge_config.yaml`
```yaml
version: "1.0"
patterns:
  temporal:
    classes: [TimeSeries, TemporalEvent]
    properties: [timestamp, duration]
performance_profiles:
  ultra_low_latency:
    targets: {read: 1, write: 8}
```

#### `quality_config.json`
```json
{
  "validation_rules": {
    "performance_compliance": {
      "enabled": true,
      "tick_limit": 8
    },
    "standard_compliance": {
      "standards": ["HIPAA", "MiFID_II"]
    }
  }
}
```

## 🎯 Best Practices

### Error Handling
- Always check return values and handle exceptions
- Use quality gates before proceeding to next steps
- Implement retry logic for network-dependent operations

### Performance
- Cache compiled ontologies when possible
- Use batch operations for multiple files
- Monitor memory usage with large ontologies

### Security
- Validate input file paths and content
- Use proper file permissions for generated code
- Audit generated code before deployment

---

This API reference provides complete documentation for integrating CNS Ontology Forge into your applications and workflows.