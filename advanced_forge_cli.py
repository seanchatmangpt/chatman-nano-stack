#!/usr/bin/env python3
"""
Advanced CNS Ontology Forge CLI
Quality-controlled ontology generation with advanced DSPy features
"""

import typer
from typing import Optional, List
from pathlib import Path
from rich.console import Console
from rich.table import Table
from rich.tree import Tree
from rich.panel import Panel
from rich.progress import Progress, SpinnerColumn, TextColumn
import json
import yaml
from datetime import datetime

from ontology_quality_control import OntologyQualityController, QualityLevel, ValidationRule
from meta_forge_orchestrator import MetaForgeOrchestrator
from ontology_meta_forge import OntologyMetaForge

app = typer.Typer(
    name="advanced-forge",
    help="Advanced CNS Ontology Forge with Quality Controls and DSPy Intelligence"
)
console = Console()

@app.command()
def validate(
    ontology_dir: Path = typer.Argument(..., help="Directory containing ontology files"),
    domain: str = typer.Option("unknown", "--domain", "-d", help="Domain type"),
    output_format: str = typer.Option("table", "--format", "-f", help="Output format (table, json, markdown)"),
    fix_auto: bool = typer.Option(False, "--auto-fix", help="Automatically fix issues where possible"),
    severity_filter: str = typer.Option("all", "--severity", "-s", help="Filter by severity (critical, warning, all)")
):
    """üîç Validate ontology files with comprehensive quality controls"""
    
    console.print(Panel.fit(
        f"[bold blue]Ontology Quality Validation[/bold blue]\n"
        f"Directory: {ontology_dir}\n"
        f"Domain: {domain}",
        border_style="blue"
    ))
    
    # Initialize quality controller
    qc = OntologyQualityController()
    
    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        console=console
    ) as progress:
        task = progress.add_task("Running comprehensive validation...", total=None)
        
        # Run validation
        report = qc.validate_ontology_suite(ontology_dir, domain)
        progress.update(task, description="Validation complete!")
    
    # Auto-fix if requested
    if fix_auto and report.issues:
        console.print("\n[yellow]Attempting auto-fixes...[/yellow]")
        auto_fixable = [i for i in report.issues if i.auto_fixable]
        if auto_fixable:
            fixes = qc.auto_fix_issues(ontology_dir, auto_fixable)
            for file_path, fix_list in fixes.items():
                console.print(f"  Fixed: {Path(file_path).name}")
                for fix in fix_list:
                    console.print(f"    ‚Ä¢ {fix}")
        else:
            console.print("  No auto-fixable issues found")
    
    # Filter issues by severity
    filtered_issues = report.issues
    if severity_filter != "all":
        filtered_issues = [i for i in report.issues if i.level.value == severity_filter]
    
    # Output results
    if output_format == "table":
        _display_validation_table(report, filtered_issues)
    elif output_format == "json":
        _output_validation_json(report, filtered_issues)
    elif output_format == "markdown":
        _output_validation_markdown(report, filtered_issues)
    
    # Return exit code based on validation result
    if not report.passed:
        console.print(f"\n[red]‚ùå Validation failed with {report.critical_count} critical issues[/red]")
        raise typer.Exit(1)
    else:
        console.print(f"\n[green]‚úÖ Validation passed! Quality score: {report.metrics['quality_score']:.1f}/100[/green]")

@app.command()
def optimize(
    ontology_dir: Path = typer.Argument(..., help="Directory containing ontology files"),
    domain: str = typer.Option("unknown", "--domain", "-d", help="Domain type"),
    target_performance: str = typer.Option("8tick", "--target", "-t", help="Performance target (8tick, 1tick, microsecond)"),
    backup: bool = typer.Option(True, "--backup", help="Create backup before optimization"),
    interactive: bool = typer.Option(False, "--interactive", "-i", help="Interactive optimization with confirmations")
):
    """‚ö° Optimize ontologies using advanced DSPy reasoning"""
    
    console.print(Panel.fit(
        f"[bold green]Ontology Optimization[/bold green]\n"
        f"Target: {target_performance} performance\n"
        f"Domain: {domain}",
        border_style="green"
    ))
    
    qc = OntologyQualityController()
    
    # Parse performance requirements
    perf_reqs = _parse_performance_target(target_performance)
    
    # Find TTL files
    ttl_files = list(ontology_dir.rglob("*.ttl"))
    
    if not ttl_files:
        console.print("[red]No TTL files found in directory[/red]")
        raise typer.Exit(1)
    
    optimizations_summary = {}
    
    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        console=console
    ) as progress:
        task = progress.add_task("Optimizing ontologies...", total=len(ttl_files))
        
        for ttl_file in ttl_files:
            # Create backup if requested
            if backup:
                backup_file = ttl_file.with_suffix('.ttl.backup')
                backup_file.write_text(ttl_file.read_text())
            
            # Optimize using DSPy
            try:
                optimized_content, optimizations = qc.optimize_ontology(ttl_file, domain, perf_reqs)
                
                if interactive:
                    console.print(f"\n[yellow]Proposed optimizations for {ttl_file.name}:[/yellow]")
                    for opt in optimizations:
                        console.print(f"  ‚Ä¢ {opt}")
                    
                    apply = typer.confirm("Apply these optimizations?")
                    if not apply:
                        progress.advance(task)
                        continue
                
                # Write optimized content
                ttl_file.write_text(optimized_content)
                optimizations_summary[str(ttl_file)] = optimizations
                
            except Exception as e:
                console.print(f"[red]Error optimizing {ttl_file.name}: {str(e)}[/red]")
            
            progress.advance(task)
    
    # Display optimization summary
    _display_optimization_summary(optimizations_summary)

@app.command()
def audit(
    ontology_dir: Path = typer.Argument(..., help="Directory containing ontology files"),
    standards: List[str] = typer.Option(["HIPAA", "ISO26262"], "--standard", "-s", help="Compliance standards to check"),
    generate_report: bool = typer.Option(True, "--report", help="Generate detailed audit report"),
    remediation_plan: bool = typer.Option(True, "--remediation", help="Generate remediation plan")
):
    """üìã Comprehensive compliance audit with remediation planning"""
    
    console.print(Panel.fit(
        f"[bold yellow]Compliance Audit[/bold yellow]\n"
        f"Standards: {', '.join(standards)}",
        border_style="yellow"
    ))
    
    qc = OntologyQualityController()
    
    # Load manifest to get domain info
    manifest_file = ontology_dir / "manifest.json"
    domain = "unknown"
    if manifest_file.exists():
        with open(manifest_file) as f:
            manifest = json.load(f)
            domain = manifest.get('domain', 'unknown')
    
    # Run full validation focusing on compliance
    report = qc.validate_ontology_suite(ontology_dir, domain)
    
    # Filter compliance issues
    compliance_issues = [i for i in report.issues if i.rule == ValidationRule.STANDARD_COMPLIANCE]
    
    if generate_report:
        _generate_audit_report(ontology_dir, compliance_issues, standards)
    
    if remediation_plan:
        _generate_remediation_plan(ontology_dir, compliance_issues, standards)
    
    # Display compliance status
    _display_compliance_status(compliance_issues, standards)

@app.command()
def benchmark(
    ontology_dir: Path = typer.Argument(..., help="Directory containing ontology files"),
    compile_to_c: bool = typer.Option(True, "--compile", help="Compile to C for benchmarking"),
    performance_gates: bool = typer.Option(True, "--gates", help="Apply performance quality gates"),
    iterations: int = typer.Option(1000000, "--iterations", "-n", help="Benchmark iterations"),
    output_format: str = typer.Option("mermaid", "--format", "-f", help="Output format (mermaid, json, table)")
):
    """üèÉ Advanced benchmarking with quality gates"""
    
    console.print(Panel.fit(
        "[bold cyan]Advanced Benchmarking Suite[/bold cyan]",
        border_style="cyan"
    ))
    
    # Validate first if quality gates enabled
    if performance_gates:
        qc = OntologyQualityController()
        report = qc.validate_ontology_suite(ontology_dir, "benchmark")
        
        if not report.passed:
            console.print("[red]‚ùå Performance quality gates failed - fix critical issues first[/red]")
            raise typer.Exit(1)
    
    # Compile to C if requested
    if compile_to_c:
        _compile_for_benchmark(ontology_dir)
    
    # Run benchmarks
    benchmark_results = _run_advanced_benchmarks(ontology_dir, iterations)
    
    # Output results
    if output_format == "mermaid":
        _output_benchmark_mermaid(benchmark_results)
    elif output_format == "json":
        _output_benchmark_json(benchmark_results)
    elif output_format == "table":
        _output_benchmark_table(benchmark_results)

@app.command()
def explain(
    ontology_file: Path = typer.Argument(..., help="Ontology file to explain"),
    depth: str = typer.Option("summary", "--depth", "-d", help="Explanation depth (summary, detailed, expert)"),
    focus: Optional[str] = typer.Option(None, "--focus", "-f", help="Focus on specific aspect (classes, properties, performance)"),
    audience: str = typer.Option("developer", "--audience", "-a", help="Target audience (developer, domain-expert, business)")
):
    """üß† AI-powered ontology explanation using DSPy reasoning"""
    
    console.print(Panel.fit(
        f"[bold magenta]AI Ontology Explanation[/bold magenta]\n"
        f"File: {ontology_file.name}\n"
        f"Depth: {depth} | Audience: {audience}",
        border_style="magenta"
    ))
    
    qc = OntologyQualityController()
    
    with open(ontology_file) as f:
        content = f.read()
    
    # Extract domain from file path
    domain = "unknown"
    if "trading" in str(ontology_file):
        domain = "trading"
    elif "healthcare" in str(ontology_file):
        domain = "healthcare"
    
    # Generate explanation using DSPy
    with console.status("Analyzing ontology with AI..."):
        explanation = qc.generate_documentation(ontology_file.parent, domain)
    
    console.print("\n[bold]AI-Generated Explanation:[/bold]\n")
    console.print(explanation)

@app.command()
def compare(
    dir1: Path = typer.Argument(..., help="First ontology directory"),
    dir2: Path = typer.Argument(..., help="Second ontology directory"),
    metric: str = typer.Option("all", "--metric", "-m", help="Comparison metric (all, performance, quality, compliance)"),
    output_format: str = typer.Option("table", "--format", "-f", help="Output format (table, json, diff)")
):
    """‚öñÔ∏è Compare ontology suites across different dimensions"""
    
    console.print(Panel.fit(
        f"[bold blue]Ontology Comparison[/bold blue]\n"
        f"Comparing: {dir1.name} vs {dir2.name}",
        border_style="blue"
    ))
    
    qc = OntologyQualityController()
    
    # Validate both directories
    report1 = qc.validate_ontology_suite(dir1, "comparison")
    report2 = qc.validate_ontology_suite(dir2, "comparison")
    
    # Generate comparison
    comparison = _generate_comparison(report1, report2, dir1, dir2, metric)
    
    # Output results
    if output_format == "table":
        _display_comparison_table(comparison)
    elif output_format == "json":
        _output_comparison_json(comparison)
    elif output_format == "diff":
        _output_comparison_diff(comparison)

@app.command()
def deploy(
    ontology_dir: Path = typer.Argument(..., help="Directory containing ontology files"),
    target_env: str = typer.Option("production", "--env", "-e", help="Target environment (development, staging, production)"),
    quality_gates: bool = typer.Option(True, "--quality-gates", help="Enforce quality gates before deployment"),
    performance_tests: bool = typer.Option(True, "--perf-tests", help="Run performance tests before deployment"),
    dry_run: bool = typer.Option(False, "--dry-run", help="Show what would be deployed without deploying")
):
    """üöÄ Deploy ontologies with comprehensive quality gates"""
    
    console.print(Panel.fit(
        f"[bold red]Ontology Deployment[/bold red]\n"
        f"Target: {target_env}\n"
        f"Quality Gates: {'‚úÖ' if quality_gates else '‚ùå'}\n"
        f"Performance Tests: {'‚úÖ' if performance_tests else '‚ùå'}",
        border_style="red"
    ))
    
    deployment_checks = []
    
    # Quality gates
    if quality_gates:
        qc = OntologyQualityController()
        report = qc.validate_ontology_suite(ontology_dir, "deployment")
        
        if report.critical_count > 0:
            console.print(f"[red]‚ùå Quality gate failed: {report.critical_count} critical issues[/red]")
            deployment_checks.append(("Quality Gate", False, f"{report.critical_count} critical issues"))
        else:
            console.print("[green]‚úÖ Quality gate passed[/green]")
            deployment_checks.append(("Quality Gate", True, f"Score: {report.metrics['quality_score']:.1f}/100"))
    
    # Performance tests
    if performance_tests:
        perf_results = _run_deployment_perf_tests(ontology_dir)
        if perf_results['passed']:
            console.print("[green]‚úÖ Performance tests passed[/green]")
            deployment_checks.append(("Performance Tests", True, "All benchmarks within limits"))
        else:
            console.print(f"[red]‚ùå Performance tests failed: {perf_results['failures']} failures[/red]")
            deployment_checks.append(("Performance Tests", False, f"{perf_results['failures']} failures"))
    
    # Display deployment readiness
    _display_deployment_readiness(deployment_checks, dry_run)
    
    # Deploy if all checks pass and not dry run
    all_passed = all(check[1] for check in deployment_checks)
    if all_passed and not dry_run:
        _execute_deployment(ontology_dir, target_env)
    elif not all_passed:
        console.print("[red]‚ùå Deployment blocked by failed checks[/red]")
        raise typer.Exit(1)

# Helper functions
def _display_validation_table(report, issues):
    """Display validation results in table format"""
    table = Table(title=f"Validation Report - Quality Score: {report.metrics['quality_score']:.1f}/100")
    table.add_column("Rule", style="cyan")
    table.add_column("Level", style="yellow")
    table.add_column("Message", style="white")
    table.add_column("File", style="dim")
    table.add_column("Auto-Fix", style="green")
    
    for issue in issues[:20]:  # Limit to first 20 issues
        level_color = "red" if issue.level == QualityLevel.CRITICAL else "yellow"
        table.add_row(
            issue.rule.value.replace('_', ' ').title(),
            f"[{level_color}]{issue.level.value.upper()}[/{level_color}]",
            issue.message[:60] + "..." if len(issue.message) > 60 else issue.message,
            Path(issue.file_path).name if issue.file_path else "-",
            "‚úÖ" if issue.auto_fixable else "‚ùå"
        )
    
    console.print(table)
    
    if len(issues) > 20:
        console.print(f"[dim]... and {len(issues) - 20} more issues[/dim]")

def _parse_performance_target(target: str) -> dict:
    """Parse performance target into requirements dict"""
    if "8tick" in target:
        return {"tick_compliance": 8, "latency_unit": "ticks"}
    elif "1tick" in target:
        return {"tick_compliance": 1, "latency_unit": "ticks"}
    elif "microsecond" in target:
        return {"latency_limit": 1, "latency_unit": "microseconds"}
    else:
        return {"latency_unit": "unknown"}

def _display_optimization_summary(optimizations_summary):
    """Display optimization summary"""
    console.print("\n[bold green]Optimization Summary:[/bold green]")
    
    for file_path, optimizations in optimizations_summary.items():
        console.print(f"\nüìù {Path(file_path).name}:")
        for opt in optimizations:
            console.print(f"  ‚Ä¢ {opt}")

def _generate_audit_report(ontology_dir, issues, standards):
    """Generate detailed audit report"""
    report_file = ontology_dir / f"audit_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
    
    content = f"""# Compliance Audit Report
Generated: {datetime.now().isoformat()}
Standards: {', '.join(standards)}

## Summary
- Total Issues: {len(issues)}
- Critical: {len([i for i in issues if i.level == QualityLevel.CRITICAL])}
- Warnings: {len([i for i in issues if i.level == QualityLevel.WARNING])}

## Issues Found
"""
    
    for issue in issues:
        content += f"""
### {issue.rule.value.replace('_', ' ').title()}
- **Level**: {issue.level.value.upper()}
- **Message**: {issue.message}
- **File**: {issue.file_path or 'N/A'}
- **Suggestion**: {issue.suggestion or 'None'}
"""
    
    report_file.write_text(content)
    console.print(f"üìã Audit report saved to: {report_file}")

def _display_compliance_status(issues, standards):
    """Display compliance status"""
    table = Table(title="Compliance Status")
    table.add_column("Standard", style="cyan")
    table.add_column("Status", style="bold")
    table.add_column("Issues", style="yellow")
    
    for standard in standards:
        standard_issues = [i for i in issues if standard in i.message]
        status = "[red]‚ùå Non-Compliant[/red]" if standard_issues else "[green]‚úÖ Compliant[/green]"
        table.add_row(standard, status, str(len(standard_issues)))
    
    console.print(table)

def _run_advanced_benchmarks(ontology_dir, iterations):
    """Run advanced benchmarking suite"""
    # Implementation would run actual benchmarks
    return {
        "total_operations": 10,
        "passed": 10,
        "failed": 0,
        "avg_latency_ns": 0.15,
        "tick_compliance": True
    }

def _output_benchmark_mermaid(results):
    """Output benchmark results as mermaid diagram"""
    console.print(f"""
```mermaid
graph TD
    A[Benchmark Results] --> B[Operations: {results['total_operations']}]
    B --> C[Passed: {results['passed']} ‚úÖ]
    B --> D[Failed: {results['failed']} ‚ùå]
    B --> E[Avg Latency: {results['avg_latency_ns']:.2f}ns]
    E --> F[8-Tick Compliant: {'‚úÖ' if results['tick_compliance'] else '‚ùå'}]
    
    classDef pass fill:#90EE90,stroke:#228B22,stroke-width:2px
    classDef metric fill:#E6F3FF,stroke:#0066CC,stroke-width:2px
    
    class C,F pass
    class A,B,E metric
```
""")

if __name__ == "__main__":
    app()