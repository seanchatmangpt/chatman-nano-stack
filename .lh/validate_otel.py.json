{
    "sourceFile": "validate_otel.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 2,
            "patches": [
                {
                    "date": 1753291133298,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1753291139092,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -234,9 +234,9 @@\n                 return {\"success\": False, \"issues\": issues, \"metrics\": metrics}\n             \n             # Validate output files\n             validation_dir = Path(\"/Users/sac/cns/validation_test\")\n-            expected_files = [\"uhft_core.c\", \"uhft_core.h\", \"uhft_core.json\", \"Makefile\"]\n+            expected_files = [\"realtime_core.c\", \"realtime_core.h\", \"realtime_core.json\", \"Makefile\"]\n             \n             for expected_file in expected_files:\n                 file_path = validation_dir / expected_file\n                 if not file_path.exists():\n"
                },
                {
                    "date": 1753291150566,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -244,11 +244,11 @@\n                 else:\n                     metrics[f\"{expected_file}_size\"] = file_path.stat().st_size\n             \n             # Validate generated C code compiles\n-            if (validation_dir / \"uhft_core.c\").exists():\n+            if (validation_dir / \"realtime_core.c\").exists():\n                 compile_result = await asyncio.create_subprocess_exec(\n-                    'gcc', '-c', 'uhft_core.c', '-o', 'uhft_core.o',\n+                    'gcc', '-c', 'realtime_core.c', '-o', 'realtime_core.o',\n                     stdout=asyncio.subprocess.PIPE,\n                     stderr=asyncio.subprocess.PIPE,\n                     cwd=str(validation_dir)\n                 )\n"
                }
            ],
            "date": 1753291133298,
            "name": "Commit-0",
            "content": "#!/usr/bin/env python3\n\"\"\"\nCNS OpenTelemetry Validation Suite\nValidates all CNS implementations against OpenTelemetry metrics\nBuilt for reliability. Designed to last.\n\"\"\"\n\nimport asyncio\nimport json\nimport subprocess\nimport time\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Dict, List, Any, Optional, Tuple\n\n# OpenTelemetry imports\nfrom opentelemetry import metrics, trace\nfrom opentelemetry.sdk.metrics import MeterProvider\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.metrics.export import ConsoleMetricExporter, PeriodicExportingMetricReader\nfrom opentelemetry.sdk.trace.export import ConsoleSpanExporter, BatchSpanProcessor\n\n\nclass OTELValidator:\n    \"\"\"Comprehensive OpenTelemetry validation for all CNS components\"\"\"\n    \n    def __init__(self):\n        self._setup_telemetry()\n        self.validation_results: List[Dict[str, Any]] = []\n        self.start_time = time.time()\n    \n    def _setup_telemetry(self) -> None:\n        \"\"\"Setup OpenTelemetry for validation testing\"\"\"\n        # Metrics setup with shorter export interval for testing\n        metric_reader = PeriodicExportingMetricReader(\n            ConsoleMetricExporter(), \n            export_interval_millis=2000\n        )\n        metrics.set_meter_provider(MeterProvider(metric_readers=[metric_reader]))\n        \n        # Tracing setup\n        trace.set_tracer_provider(TracerProvider())\n        tracer_provider = trace.get_tracer_provider()\n        span_processor = BatchSpanProcessor(ConsoleSpanExporter())\n        tracer_provider.add_span_processor(span_processor)\n        \n        self.meter = metrics.get_meter(\"cns.validator\", version=\"1.0.0\")\n        self.tracer = trace.get_tracer(\"cns.validator\")\n        \n        # Validation metrics\n        self.validation_counter = self.meter.create_counter(\n            name=\"validations_total\",\n            description=\"Total number of validations performed\",\n        )\n        \n        self.validation_duration = self.meter.create_histogram(\n            name=\"validation_duration_ms\",\n            description=\"Validation duration in milliseconds\",\n            unit=\"ms\"\n        )\n        \n        self.success_rate_gauge = self.meter.create_gauge(\n            name=\"validation_success_rate\",\n            description=\"Validation success rate percentage\",\n            unit=\"%\"\n        )\n    \n    async def run_comprehensive_validation(self) -> Dict[str, Any]:\n        \"\"\"Run comprehensive validation of all CNS components against OTEL\"\"\"\n        print(\"üî¨ CNS OpenTelemetry Validation Suite\")\n        print(\"=====================================\")\n        print(f\"Started: {datetime.now().isoformat()}\")\n        print()\n        \n        with self.tracer.start_as_current_span(\"comprehensive_validation\") as root_span:\n            # Test all components\n            validations = [\n                (\"CNS Status Command\", self._validate_cns_status),\n                (\"OWL Compiler\", self._validate_owl_compiler),\n                (\"Benchmark System\", self._validate_benchmark_system),\n                (\"Performance Monitor\", self._validate_performance_monitor),\n                (\"Generated C Code\", self._validate_generated_code),\n                (\"OTEL Integration\", self._validate_otel_integration)\n            ]\n            \n            for name, validator in validations:\n                await self._run_validation_test(name, validator)\n            \n            # Generate final validation report\n            report = self._generate_validation_report()\n            \n            root_span.set_attributes({\n                \"total_validations\": len(validations),\n                \"successful_validations\": len([r for r in self.validation_results if r[\"success\"]]),\n                \"validation_duration_s\": time.time() - self.start_time\n            })\n            \n            return report\n    \n    async def _run_validation_test(self, name: str, validator_func) -> None:\n        \"\"\"Run a single validation test with OTEL tracing\"\"\"\n        start_time = time.time()\n        \n        with self.tracer.start_as_current_span(f\"validate_{name.lower().replace(' ', '_')}\") as span:\n            try:\n                print(f\"üß™ Validating {name}...\")\n                result = await validator_func()\n                \n                duration_ms = (time.time() - start_time) * 1000\n                \n                self.validation_results.append({\n                    \"name\": name,\n                    \"success\": result[\"success\"],\n                    \"duration_ms\": duration_ms,\n                    \"metrics\": result.get(\"metrics\", {}),\n                    \"issues\": result.get(\"issues\", []),\n                    \"timestamp\": datetime.now().isoformat()\n                })\n                \n                # Record OTEL metrics\n                self.validation_counter.add(1, {\"component\": name.lower().replace(\" \", \"_\"), \"status\": \"passed\" if result[\"success\"] else \"failed\"})\n                self.validation_duration.record(duration_ms, {\"component\": name.lower().replace(\" \", \"_\")})\n                \n                span.set_attributes({\n                    \"validation.success\": result[\"success\"],\n                    \"validation.duration_ms\": duration_ms,\n                    \"validation.metrics_count\": len(result.get(\"metrics\", {})),\n                    \"validation.issues_count\": len(result.get(\"issues\", []))\n                })\n                \n                status = \"‚úÖ PASS\" if result[\"success\"] else \"‚ùå FAIL\"\n                print(f\"   {status} ({duration_ms:.1f}ms)\")\n                \n                if result.get(\"issues\"):\n                    for issue in result[\"issues\"]:\n                        print(f\"   ‚ö†Ô∏è  {issue}\")\n                \n            except Exception as e:\n                duration_ms = (time.time() - start_time) * 1000\n                \n                self.validation_results.append({\n                    \"name\": name,\n                    \"success\": False,\n                    \"duration_ms\": duration_ms,\n                    \"error\": str(e),\n                    \"timestamp\": datetime.now().isoformat()\n                })\n                \n                self.validation_counter.add(1, {\"component\": name.lower().replace(\" \", \"_\"), \"status\": \"error\"})\n                self.validation_duration.record(duration_ms, {\"component\": name.lower().replace(\" \", \"_\")})\n                \n                span.set_attributes({\n                    \"validation.success\": False,\n                    \"validation.error\": str(e),\n                    \"validation.duration_ms\": duration_ms\n                })\n                \n                print(f\"   ‚ùå ERROR ({duration_ms:.1f}ms): {e}\")\n    \n    async def _validate_cns_status(self) -> Dict[str, Any]:\n        \"\"\"Validate CNS status command OTEL integration\"\"\"\n        issues = []\n        metrics = {}\n        \n        try:\n            # Test cns-status command\n            result = await asyncio.create_subprocess_exec(\n                'uv', 'run', 'python', 'cns_status.py', '--format', 'json',\n                stdout=asyncio.subprocess.PIPE,\n                stderr=asyncio.subprocess.PIPE,\n                cwd='/Users/sac/cns'\n            )\n            stdout, stderr = await result.communicate()\n            \n            if result.returncode != 0:\n                issues.append(f\"CNS status command failed with code {result.returncode}\")\n                return {\"success\": False, \"issues\": issues}\n            \n            # Parse JSON output  \n            try:\n                status_data = json.loads(stdout.decode())\n                metrics[\"health_score\"] = status_data.get(\"health_score\", 0)\n                metrics[\"status\"] = status_data.get(\"status\", \"UNKNOWN\")\n                \n                # Validate expected fields\n                required_fields = [\"timestamp\", \"health_score\", \"status\", \"system\", \"cns\", \"performance\"]\n                for field in required_fields:\n                    if field not in status_data:\n                        issues.append(f\"Missing required field: {field}\")\n                \n                # Validate OTEL-related fields\n                if \"performance\" in status_data:\n                    perf = status_data[\"performance\"]\n                    if \"avg_latency_ms\" not in perf:\n                        issues.append(\"Missing OTEL performance metric: avg_latency_ms\")\n                    if \"throughput_ops_sec\" not in perf:\n                        issues.append(\"Missing OTEL performance metric: throughput_ops_sec\")\n                \n            except json.JSONDecodeError:\n                issues.append(\"CNS status output is not valid JSON\")\n                return {\"success\": False, \"issues\": issues}\n            \n        except Exception as e:\n            issues.append(f\"CNS status validation error: {e}\")\n            return {\"success\": False, \"issues\": issues}\n        \n        return {\"success\": len(issues) == 0, \"issues\": issues, \"metrics\": metrics}\n    \n    async def _validate_owl_compiler(self) -> Dict[str, Any]:\n        \"\"\"Validate OWL compiler functionality and output\"\"\"\n        issues = []\n        metrics = {}\n        \n        try:\n            # Test compilation\n            start_time = time.time()\n            result = await asyncio.create_subprocess_exec(\n                'uv', 'run', 'python', 'owl_compiler.py', \n                'ontologies/generated/realtime/realtime_core.ttl', '--output', 'validation_test',\n                stdout=asyncio.subprocess.PIPE,\n                stderr=asyncio.subprocess.PIPE,\n                cwd='/Users/sac/cns'\n            )\n            stdout, stderr = await result.communicate()\n            compilation_time = time.time() - start_time\n            \n            metrics[\"compilation_time_s\"] = compilation_time\n            metrics[\"return_code\"] = result.returncode\n            \n            if result.returncode != 0:\n                issues.append(f\"OWL compiler failed with code {result.returncode}\")\n                if stderr:\n                    issues.append(f\"Stderr: {stderr.decode()[:200]}...\")\n                return {\"success\": False, \"issues\": issues, \"metrics\": metrics}\n            \n            # Validate output files\n            validation_dir = Path(\"/Users/sac/cns/validation_test\")\n            expected_files = [\"uhft_core.c\", \"uhft_core.h\", \"uhft_core.json\", \"Makefile\"]\n            \n            for expected_file in expected_files:\n                file_path = validation_dir / expected_file\n                if not file_path.exists():\n                    issues.append(f\"Missing expected output file: {expected_file}\")\n                else:\n                    metrics[f\"{expected_file}_size\"] = file_path.stat().st_size\n            \n            # Validate generated C code compiles\n            if (validation_dir / \"uhft_core.c\").exists():\n                compile_result = await asyncio.create_subprocess_exec(\n                    'gcc', '-c', 'uhft_core.c', '-o', 'uhft_core.o',\n                    stdout=asyncio.subprocess.PIPE,\n                    stderr=asyncio.subprocess.PIPE,\n                    cwd=str(validation_dir)\n                )\n                await compile_result.wait()\n                \n                if compile_result.returncode != 0:\n                    issues.append(\"Generated C code does not compile\")\n                else:\n                    metrics[\"c_compilation_success\"] = True\n            \n            # Cleanup\n            if validation_dir.exists():\n                import shutil\n                shutil.rmtree(validation_dir)\n        \n        except Exception as e:\n            issues.append(f\"OWL compiler validation error: {e}\")\n            return {\"success\": False, \"issues\": issues, \"metrics\": metrics}\n        \n        return {\"success\": len(issues) == 0, \"issues\": issues, \"metrics\": metrics}\n    \n    async def _validate_benchmark_system(self) -> Dict[str, Any]:\n        \"\"\"Validate benchmark system OTEL integration\"\"\"\n        issues = []\n        metrics = {}\n        \n        try:\n            # Test benchmark system\n            start_time = time.time()\n            result = await asyncio.create_subprocess_exec(\n                'uv', 'run', 'python', 'run_benchmark.py',\n                stdout=asyncio.subprocess.PIPE,\n                stderr=asyncio.subprocess.PIPE,\n                cwd='/Users/sac/cns'\n            )\n            stdout, stderr = await result.communicate()\n            benchmark_time = time.time() - start_time\n            \n            metrics[\"benchmark_time_s\"] = benchmark_time\n            metrics[\"return_code\"] = result.returncode\n            \n            if result.returncode != 0:\n                issues.append(f\"Benchmark system failed with code {result.returncode}\")\n                return {\"success\": False, \"issues\": issues, \"metrics\": metrics}\n            \n            output = stdout.decode()\n            \n            # Validate OTEL integration in output\n            if \"üìä OTEL Metrics:\" not in output:\n                issues.append(\"Missing OTEL metrics section in benchmark output\")\n            \n            # Validate Mermaid diagrams\n            mermaid_count = output.count(\"```mermaid\")\n            metrics[\"mermaid_diagrams_count\"] = mermaid_count\n            \n            if mermaid_count < 3:\n                issues.append(f\"Expected at least 3 Mermaid diagrams, found {mermaid_count}\")\n            \n            # Parse performance score\n            if \"Performance Score:\" in output:\n                try:\n                    score_line = [line for line in output.split('\\n') if 'Performance Score:' in line][0]\n                    score = float(score_line.split('Performance Score:')[1].strip().split('/')[0])\n                    metrics[\"performance_score\"] = score\n                    \n                    if score < 80.0:\n                        issues.append(f\"Performance score too low: {score}/100\")\n                        \n                except:\n                    issues.append(\"Could not parse performance score from benchmark output\")\n            else:\n                issues.append(\"Performance score not found in benchmark output\")\n        \n        except Exception as e:\n            issues.append(f\"Benchmark validation error: {e}\")\n            return {\"success\": False, \"issues\": issues, \"metrics\": metrics}\n        \n        return {\"success\": len(issues) == 0, \"issues\": issues, \"metrics\": metrics}\n    \n    async def _validate_performance_monitor(self) -> Dict[str, Any]:\n        \"\"\"Validate performance monitor OTEL integration\"\"\"\n        issues = []\n        metrics = {}\n        \n        try:\n            # Test performance monitor with short duration\n            start_time = time.time()\n            result = await asyncio.create_subprocess_exec(\n                'uv', 'run', 'python', 'cns_monitor.py', '--duration', '1', '--interval', '30',\n                stdout=asyncio.subprocess.PIPE,\n                stderr=asyncio.subprocess.PIPE,\n                cwd='/Users/sac/cns'\n            )\n            \n            # Wait for completion with timeout\n            try:\n                stdout, stderr = await asyncio.wait_for(result.communicate(), timeout=90.0)\n            except asyncio.TimeoutError:\n                result.terminate()\n                issues.append(\"Performance monitor validation timed out\")\n                return {\"success\": False, \"issues\": issues, \"metrics\": metrics}\n            \n            monitor_time = time.time() - start_time\n            metrics[\"monitor_time_s\"] = monitor_time\n            metrics[\"return_code\"] = result.returncode\n            \n            if result.returncode not in [0, -2]:  # 0 = success, -2 = KeyboardInterrupt\n                issues.append(f\"Performance monitor failed with code {result.returncode}\")\n            \n            output = stdout.decode()\n            \n            # Validate OTEL metrics presence\n            if \"cns.monitor\" not in output:\n                issues.append(\"OTEL metrics from cns.monitor not found\")\n            \n            # Validate performance monitoring features\n            expected_features = [\n                \"CNS PERFORMANCE MONITOR\",\n                \"system_cpu_percent\",\n                \"system_memory_percent\", \n                \"cns_health_score\",\n                \"compilation_duration_seconds\"\n            ]\n            \n            for feature in expected_features:\n                if feature not in output:\n                    issues.append(f\"Missing expected feature: {feature}\")\n            \n            # Count OTEL metric exports\n            otel_exports = output.count('\"resource_metrics\"')\n            metrics[\"otel_exports_count\"] = otel_exports\n            \n            if otel_exports < 1:\n                issues.append(\"No OTEL metric exports found\")\n        \n        except Exception as e:\n            issues.append(f\"Performance monitor validation error: {e}\")\n            return {\"success\": False, \"issues\": issues, \"metrics\": metrics}\n        \n        return {\"success\": len(issues) == 0, \"issues\": issues, \"metrics\": metrics}\n    \n    async def _validate_generated_code(self) -> Dict[str, Any]:\n        \"\"\"Validate generated C code functionality\"\"\"\n        issues = []\n        metrics = {}\n        \n        try:\n            binary_path = Path(\"/Users/sac/cns/live_system/owl_ontology\")\n            \n            if not binary_path.exists():\n                issues.append(\"Generated binary not found\")\n                return {\"success\": False, \"issues\": issues, \"metrics\": metrics}\n            \n            metrics[\"binary_size\"] = binary_path.stat().st_size\n            \n            # Test self-test functionality\n            result = await asyncio.create_subprocess_exec(\n                str(binary_path), '--self-test',\n                stdout=asyncio.subprocess.PIPE,\n                stderr=asyncio.subprocess.PIPE\n            )\n            stdout, stderr = await result.communicate()\n            \n            output = stdout.decode()\n            metrics[\"self_test_return_code\"] = result.returncode\n            \n            # Parse test results\n            if \"Test Results:\" in output:\n                try:\n                    result_line = [line for line in output.split('\\n') if 'Test Results:' in line][0]\n                    passed_str = result_line.split('Test Results:')[1].strip().split()[0]\n                    passed, total = map(int, passed_str.split('/'))\n                    metrics[\"tests_passed\"] = passed\n                    metrics[\"tests_total\"] = total\n                    \n                    if passed < 3:  # At least core tests should pass\n                        issues.append(f\"Too few tests passed: {passed}/{total}\")\n                        \n                except:\n                    issues.append(\"Could not parse test results\")\n            else:\n                issues.append(\"Test results not found in self-test output\")\n            \n            # Test other modes\n            modes = [\"--help\", \"--deploy-production\"]\n            for mode in modes:\n                mode_result = await asyncio.create_subprocess_exec(\n                    str(binary_path), mode,\n                    stdout=asyncio.subprocess.PIPE,\n                    stderr=asyncio.subprocess.PIPE\n                )\n                await mode_result.wait()\n                metrics[f\"{mode}_return_code\"] = mode_result.returncode\n                \n                if mode_result.returncode != 0:\n                    issues.append(f\"Mode {mode} failed with code {mode_result.returncode}\")\n        \n        except Exception as e:\n            issues.append(f\"Generated code validation error: {e}\")\n            return {\"success\": False, \"issues\": issues, \"metrics\": metrics}\n        \n        return {\"success\": len(issues) == 0, \"issues\": issues, \"metrics\": metrics}\n    \n    async def _validate_otel_integration(self) -> Dict[str, Any]:\n        \"\"\"Validate overall OTEL integration across all components\"\"\"\n        issues = []\n        metrics = {}\n        \n        try:\n            # Test that OTEL is properly configured\n            from opentelemetry import metrics as otel_metrics\n            from opentelemetry import trace as otel_trace\n            \n            # Check meter provider\n            meter_provider = otel_metrics.get_meter_provider()\n            if not meter_provider:\n                issues.append(\"OTEL meter provider not configured\")\n            else:\n                metrics[\"meter_provider_configured\"] = True\n            \n            # Check tracer provider  \n            tracer_provider = otel_trace.get_tracer_provider()\n            if not tracer_provider:\n                issues.append(\"OTEL tracer provider not configured\")\n            else:\n                metrics[\"tracer_provider_configured\"] = True\n            \n            # Test metrics creation\n            test_meter = otel_metrics.get_meter(\"validation.test\")\n            test_counter = test_meter.create_counter(\"test_counter\")\n            test_counter.add(1, {\"validation\": \"otel_integration\"})\n            metrics[\"test_metric_created\"] = True\n            \n            # Test tracing\n            test_tracer = otel_trace.get_tracer(\"validation.test\")\n            with test_tracer.start_as_current_span(\"test_span\") as span:\n                span.set_attribute(\"validation\", \"otel_integration\")\n                metrics[\"test_span_created\"] = True\n            \n        except Exception as e:\n            issues.append(f\"OTEL integration validation error: {e}\")\n            return {\"success\": False, \"issues\": issues, \"metrics\": metrics}\n        \n        return {\"success\": len(issues) == 0, \"issues\": issues, \"metrics\": metrics}\n    \n    def _generate_validation_report(self) -> Dict[str, Any]:\n        \"\"\"Generate comprehensive validation report with Mermaid diagrams\"\"\"\n        total_validations = len(self.validation_results)\n        successful_validations = len([r for r in self.validation_results if r[\"success\"]])\n        success_rate = (successful_validations / total_validations * 100) if total_validations > 0 else 0\n        \n        # Update success rate gauge\n        self.success_rate_gauge.set(success_rate)\n        \n        print(\"\\n\" + \"=\"*80)\n        print(\"üèÅ CNS OPENTELEMETRY VALIDATION REPORT\")\n        print(\"=\"*80)\n        print(f\"Validation Duration: {time.time() - self.start_time:.1f}s\")\n        print(f\"Total Validations: {total_validations}\")\n        print(f\"Successful: {successful_validations}\")\n        print(f\"Failed: {total_validations - successful_validations}\")\n        print(f\"Success Rate: {success_rate:.1f}%\")\n        print()\n        \n        # Component results\n        for result in self.validation_results:\n            status = \"‚úÖ PASS\" if result[\"success\"] else \"‚ùå FAIL\"\n            duration = result[\"duration_ms\"]\n            print(f\"{status} {result['name']:25} ({duration:.1f}ms)\")\n            \n            if not result[\"success\"]:\n                issues = result.get(\"issues\", [])\n                if result.get(\"error\"):\n                    issues.append(result[\"error\"])\n                for issue in issues:\n                    print(f\"     ‚ö†Ô∏è  {issue}\")\n        \n        # Mermaid validation flow diagram\n        print(\"\\n```mermaid\")\n        print(\"graph TD\")\n        print(\"    A[CNS OTEL Validation Suite] --> B[CNS Status]\")\n        print(\"    A --> C[OWL Compiler]\")\n        print(\"    A --> D[Benchmark System]\")\n        print(\"    A --> E[Performance Monitor]\")\n        print(\"    A --> F[Generated C Code]\")\n        print(\"    A --> G[OTEL Integration]\")\n        \n        for result in self.validation_results:\n            node_id = result[\"name\"].replace(\" \", \"\").replace(\"CNS\", \"\").replace(\"OWL\", \"\")[:1]\n            status = \"PASS\" if result[\"success\"] else \"FAIL\"\n            duration = result[\"duration_ms\"]\n            print(f\"    {node_id} --> {node_id}1[{status}]\")\n            print(f\"    {node_id}1 --> {node_id}2[{duration:.0f}ms]\")\n            if result[\"success\"]:\n                print(f\"    class {node_id}1 pass\")\n            else:\n                print(f\"    class {node_id}1 fail\")\n        \n        print(\"    classDef pass fill:#90EE90\")\n        print(\"    classDef fail fill:#FFB6C1\")\n        print(\"```\")\n        \n        # Success rate pie chart\n        print(\"\\n```mermaid\")\n        print(\"pie title CNS OTEL Validation Results\")\n        print(f'    \"Passed\" : {successful_validations}')\n        print(f'    \"Failed\" : {total_validations - successful_validations}')\n        print(\"```\")\n        \n        # Performance timeline\n        print(\"\\n```mermaid\")\n        print(\"timeline\")\n        print(\"    title CNS Validation Timeline\")\n        for result in self.validation_results:\n            status = \"‚úÖ\" if result[\"success\"] else \"‚ùå\"\n            duration = result[\"duration_ms\"]\n            name = result[\"name\"].replace(\"CNS \", \"\").replace(\" System\", \"\")\n            print(f\"    {name} : {status} {duration:.0f}ms\")\n        print(\"```\")\n        \n        # Generate final status\n        if success_rate == 100.0:\n            print(f\"\\nüéâ ALL VALIDATIONS PASSED - CNS is FULLY VALIDATED against OpenTelemetry\")\n            final_status = \"OPTIMAL\"\n        elif success_rate >= 80.0:\n            print(f\"\\n‚ö†Ô∏è  MOST VALIDATIONS PASSED - CNS is MOSTLY VALIDATED ({success_rate:.1f}%)\")\n            final_status = \"ACCEPTABLE\"\n        else:\n            print(f\"\\nüö® VALIDATION FAILURES - CNS needs attention ({success_rate:.1f}% success rate)\")\n            final_status = \"CRITICAL\"\n        \n        return {\n            \"validation_summary\": {\n                \"total_validations\": total_validations,\n                \"successful_validations\": successful_validations,\n                \"success_rate\": success_rate,\n                \"duration_seconds\": time.time() - self.start_time,\n                \"final_status\": final_status\n            },\n            \"validation_results\": self.validation_results,\n            \"timestamp\": datetime.now().isoformat()\n        }\n\n\nasync def main():\n    \"\"\"Main entry point\"\"\"\n    validator = OTELValidator()\n    \n    try:\n        await validator.run_comprehensive_validation()\n    except KeyboardInterrupt:\n        print(\"\\nüëã Validation stopped by user\")\n    except Exception as e:\n        print(f\"\\nüí• Validation failed with error: {e}\")\n        raise\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
        }
    ]
}