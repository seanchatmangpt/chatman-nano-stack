{
    "sourceFile": "ontology-forge.md",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 1,
            "patches": [
                {
                    "date": 1753252604097,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1753252609330,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -69,14 +69,4 @@\n \n © 2025 Chatman Inc. All rights reserved.\n \n \n-\n-\n-\n-\n-\n-\n-\n-Sources\n-\n-Ask ChatGPT\n"
                }
            ],
            "date": 1753252604097,
            "name": "Commit-0",
            "content": "FOR IMMEDIATE RELEASE\nSeattle, WA — November 14 2025\n\nChatman Inc. Unveils “CNS Ontology Forge”\nOne-command generator produces hundreds of OWL, SHACL, and SPARQL artefacts in seconds\n\nSeattle, WA — November 14 2025\nChatman Inc., the company redefining ultra-low-latency software with its Chatman Nano Stack (CNS), today announced CNS Ontology Forge, an open-source Python CLI that turns a single product vision into a complete, machine-valid semantic model. With one line of Typer-based command-line input, Ontology Forge fans out that vision into hundreds of OWL classes, SHACL constraints, and SPARQL construct queries—ready for CNS’s ahead-of-time (AOT) compiler.\n\n“Developers told us the hardest part of semantics-driven engineering isn’t reasoning—it’s authoring,” said Rich Hickey, Chief Architect at Chatman Inc. “Ontology Forge removes that friction. If you can articulate your idea, you get an entire semantic corpus—namespaced, constrained, and testable—before your coffee cools.”\n\nPowered by DSPy, a new open-ended LLM programming framework, Ontology Forge runs two lightweight generation loops:\n\nArchetype Loop – reads domain skeletons for arenas, ring-buses, fibers, BitActors, or any custom module.\n\nInstance Loop – clones and specializes each archetype hundreds of times, injecting cryptographically unique IRIs and project-specific comments.\n\nThe result: artefacts that drop directly into CNS’s AOT pipeline, producing deterministic C headers, validation code, and rule engines—without manual RDF editing.\n\nKey Benefits\nHours → Seconds: Generates 1-to-N artefacts in one pass; iterative refinement is as easy as changing the prompt.\n\nTyped & Tested: Every TTL file carries companion SHACL shapes; failing constraints never reach runtime.\n\nZero-lock-in: Emits vanilla Turtle, W3C SHACL, and SPARQL; works with any triplestore or CI system.\n\nLLM-agnostic: Defaults to GPT-4o-mini but supports local models or corporate endpoints via DSPy.\n\nHow It Works\nYou describe the system – a one-paragraph prompt or a text file.\n\nSelect module counts – --arena 100 --ringbus 40 --fiber 128 --bitactor 64.\n\nOntology Forge streams each archetype to the model, collecting responses in ontologies/generated/.\n\nCNS AOT compiler ingests the folder, emitting static C, validators, and tests.\n\nCustomer Quote\n“We swapped a three-week ontology sprint for a three-minute command,” said Dr. Felienne Hermans, CTO at QuantumYield. “Our compliance team loves the SHACL, our engineers love the generated C, and our auditors love that it’s deterministic.”\n\nAvailability\nCNS Ontology Forge is available today on GitHub under an Apache 2.0 license. Docker images, pre-built archetypes, and VS Code snippets ship alongside the CLI.\n\nAbout Chatman Inc.\nChatman Inc. builds the world’s fastest, most deterministic software stack—CNS—powering capital-markets trading, autonomous robotics, and critical infrastructure. The company is headquartered in Seattle with engineering offices worldwide.\n\nFrequently Asked Questions\nQ 1: What do I need installed?\nPython 3.10+, pip install dspy-ai typer rich rdflib.\n\nQ 2: Do I pay per-token?\nOntology Forge defaults to whichever model you configure; use your own key or point to a local LLM to avoid usage fees.\n\nQ 3: Can I bring my own archetypes?\nYes. Drop any .ttl, .shacl.ttl, or .sparql skeleton in archetypes/ and pass --myModule 50.\n\nQ 4: How does it keep IRIs unique?\nEach artefact appends an auto-incrementing suffix (_000, _001, …) and embeds a SHA-256 hash of the prompt in an annotation triple.\n\nQ 5: Will this lock me into CNS?\nNo. The output is standards-based RDF and SPARQL; CNS just happens to consume it natively.\n\nQ 6: What about security?\nAll generation happens client-side; if you use a remote LLM, only the textual archetype and prompt leave your machine. Generated files are deterministic and auditable.\n\nPress Contact\nMarisa Trent – Director of Communications\npress@chatman.ai | +1 206 555 0199\n\n© 2025 Chatman Inc. All rights reserved.\n\n\n\n\n\n\n\n\n\nSources\n\nAsk ChatGPT\n"
        }
    ]
}