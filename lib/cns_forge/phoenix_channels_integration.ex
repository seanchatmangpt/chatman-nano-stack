defmodule CnsForge.PhoenixChannelsIntegration do
  @moduledoc """
  ðŸ”¥ PHOENIX CHANNELS INTEGRATION FOR ASH REACTOR PIPELINE
  Real-time notifications for every pipeline stage:
  typer â†’ turtle â†’ ttl2dspy â†’ BitActor â†’ Erlang â†’ Ash â†’ Reactor â†’ k8s
  
  80/20 APPROACH: Maximum real-time visibility with minimal complexity
  """
  
  use Application
  require Logger
  
  def start(_type, _args) do
    children = [
      # PubSub for real-time events
      {Phoenix.PubSub, name: CnsForge.PubSub},
      
      # Channel supervisors
      CnsForge.PipelineChannelSupervisor,
      CnsForge.ReactorStepChannelSupervisor,
      CnsForge.TelemetryChannelSupervisor,
      
      # Real-time event processors
      CnsForge.PipelineEventProcessor,
      CnsForge.ReactorStepNotifier,
      CnsForge.TelemetryStreamer,
      
      # WebSocket endpoint
      {Phoenix.Endpoint, CnsForge.ChannelEndpoint}
    ]
    
    opts = [strategy: :one_for_one, name: CnsForge.ChannelsSupervisor]
    Supervisor.start_link(children, opts)
  end
end

defmodule CnsForge.ChannelEndpoint do
  @moduledoc """
  Phoenix Endpoint for real-time channel communication
  """
  
  use Phoenix.Endpoint, otp_app: :cns_forge
  
  # WebSocket configuration
  socket "/socket", CnsForge.UserSocket,
    websocket: true,
    longpoll: false
  
  # Serve static files
  plug Plug.Static,
    at: "/",
    from: :cns_forge,
    gzip: false,
    only: ~w(assets fonts images favicon.ico robots.txt)
  
  # CORS for cross-origin requests
  plug CORSPlug
  
  # Request parsing
  plug Plug.RequestId
  plug Plug.Telemetry, event_prefix: [:phoenix, :endpoint]
  
  plug Plug.Parsers,
    parsers: [:urlencoded, :multipart, :json],
    pass: ["*/*"],
    json_decoder: Phoenix.json_library()
  
  plug Plug.MethodOverride
  plug Plug.Head
  
  # Router for channel endpoints
  plug CnsForge.ChannelRouter
end

defmodule CnsForge.UserSocket do
  @moduledoc """
  User socket for Phoenix Channels
  Handles authentication and channel routing
  """
  
  use Phoenix.Socket
  
  # Channels for different aspects of the pipeline
  channel "pipeline:*", CnsForge.PipelineChannel
  channel "reactor:*", CnsForge.ReactorStepChannel  
  channel "telemetry:*", CnsForge.TelemetryChannel
  channel "notifications:*", CnsForge.NotificationChannel
  channel "system:*", CnsForge.SystemChannel
  
  # Socket ID for tracking connections
  @impl true
  def id(socket), do: "user_socket:#{socket.assigns.user_id}"
  
  # Connection authentication
  @impl true
  def connect(%{"token" => token}, socket, _connect_info) do
    case verify_token(token) do
      {:ok, user_id} ->
        socket = assign(socket, :user_id, user_id)
        {:ok, socket}
      
      {:error, _reason} ->
        :error
    end
  end
  
  def connect(_params, _socket, _connect_info) do
    :error
  end
  
  defp verify_token(token) do
    # Simplified token verification
    # In production, use proper JWT verification
    case token do
      "pipeline_monitor_token" -> {:ok, "pipeline_monitor"}
      "admin_token" -> {:ok, "admin"}
      _ -> {:error, "invalid_token"}
    end
  end
end

defmodule CnsForge.PipelineChannel do
  @moduledoc """
  Real-time pipeline monitoring channel
  Streams events from: typer â†’ turtle â†’ ttl2dspy â†’ BitActor â†’ Erlang â†’ Ash â†’ Reactor â†’ k8s
  """
  
  use Phoenix.Channel
  require Logger
  
  # Join pipeline monitoring
  def join("pipeline:" <> pipeline_id, _payload, socket) do
    Logger.info("Client joined pipeline channel: #{pipeline_id}")
    
    # Subscribe to pipeline events
    Phoenix.PubSub.subscribe(CnsForge.PubSub, "pipeline_events:#{pipeline_id}")
    Phoenix.PubSub.subscribe(CnsForge.PubSub, "pipeline_telemetry:#{pipeline_id}")
    
    # Send current pipeline status
    current_status = get_pipeline_status(pipeline_id)
    push(socket, "pipeline_status", current_status)
    
    socket = assign(socket, :pipeline_id, pipeline_id)
    {:ok, socket}
  end
  
  # Handle stage transition events
  def handle_info({:stage_transition, stage, from_status, to_status, data}, socket) do
    push(socket, "stage_transition", %{
      stage: stage,
      from: from_status, 
      to: to_status,
      data: data,
      timestamp: DateTime.utc_now()
    })
    {:noreply, socket}
  end
  
  # Handle stage completion events
  def handle_info({:stage_completed, stage, result, metadata}, socket) do
    push(socket, "stage_completed", %{
      stage: stage,
      result: result,
      metadata: metadata,
      timestamp: DateTime.utc_now()
    })
    {:noreply, socket}
  end
  
  # Handle pipeline failure events
  def handle_info({:pipeline_failure, stage, error, context}, socket) do
    push(socket, "pipeline_failure", %{
      failed_stage: stage,
      error: error,
      context: context,
      timestamp: DateTime.utc_now(),
      severity: "critical"
    })
    {:noreply, socket}
  end
  
  # Handle telemetry updates
  def handle_info({:telemetry_update, metrics}, socket) do
    push(socket, "telemetry", %{
      metrics: metrics,
      timestamp: DateTime.utc_now()
    })
    {:noreply, socket}
  end
  
  # Client requests pipeline restart
  def handle_in("restart_pipeline", %{"stage" => stage}, socket) do
    pipeline_id = socket.assigns.pipeline_id
    
    case restart_pipeline_from_stage(pipeline_id, stage) do
      {:ok, _} ->
        broadcast!(socket, "pipeline_restarted", %{
          stage: stage,
          restarted_by: socket.assigns.user_id
        })
        {:reply, {:ok, %{status: "restarted"}}, socket}
      
      {:error, reason} ->
        {:reply, {:error, %{reason: reason}}, socket}
    end
  end
  
  # Client requests pipeline pause
  def handle_in("pause_pipeline", _payload, socket) do
    pipeline_id = socket.assigns.pipeline_id
    
    case pause_pipeline(pipeline_id) do
      {:ok, _} ->
        broadcast!(socket, "pipeline_paused", %{
          paused_by: socket.assigns.user_id
        })
        {:reply, {:ok, %{status: "paused"}}, socket}
      
      {:error, reason} ->
        {:reply, {:error, %{reason: reason}}, socket}
    end
  end
  
  defp get_pipeline_status(pipeline_id) do
    # Get current status of each stage
    %{
      pipeline_id: pipeline_id,
      stages: %{
        typer: get_stage_status("typer", pipeline_id),
        turtle: get_stage_status("turtle", pipeline_id),
        ttl2dspy: get_stage_status("ttl2dspy", pipeline_id),
        bitactor: get_stage_status("bitactor", pipeline_id),
        erlang: get_stage_status("erlang", pipeline_id),
        ash: get_stage_status("ash", pipeline_id),
        reactor: get_stage_status("reactor", pipeline_id),
        k8s: get_stage_status("k8s", pipeline_id)
      },
      overall_status: calculate_overall_status(pipeline_id),
      last_updated: DateTime.utc_now()
    }
  end
  
  defp get_stage_status(stage, _pipeline_id) do
    # Simplified status - in production, query actual stage state
    %{
      status: "ready",
      last_execution: DateTime.add(DateTime.utc_now(), -300, :second),
      success_rate: 0.95,
      avg_duration_ms: 150
    }
  end
  
  defp calculate_overall_status(_pipeline_id) do
    "ready"
  end
  
  defp restart_pipeline_from_stage(pipeline_id, stage) do
    Logger.info("Restarting pipeline #{pipeline_id} from stage #{stage}")
    # Implementation would trigger actual pipeline restart
    {:ok, %{restarted: true}}
  end
  
  defp pause_pipeline(pipeline_id) do
    Logger.info("Pausing pipeline #{pipeline_id}")
    # Implementation would pause actual pipeline
    {:ok, %{paused: true}}
  end
end

defmodule CnsForge.ReactorStepChannel do
  @moduledoc """
  Real-time Ash Reactor step notifications
  Provides detailed step-by-step execution visibility
  """
  
  use Phoenix.Channel
  require Logger
  
  def join("reactor:" <> reactor_name, _payload, socket) do
    Logger.info("Client joined reactor channel: #{reactor_name}")
    
    # Subscribe to reactor step events
    Phoenix.PubSub.subscribe(CnsForge.PubSub, "reactor_steps:#{reactor_name}")
    Phoenix.PubSub.subscribe(CnsForge.PubSub, "reactor_errors:#{reactor_name}")
    
    # Send current reactor status
    current_status = get_reactor_status(reactor_name)
    push(socket, "reactor_status", current_status)
    
    socket = assign(socket, :reactor_name, reactor_name)
    {:ok, socket}
  end
  
  # Handle step start events
  def handle_info({:step_started, step_name, input_data}, socket) do
    push(socket, "step_started", %{
      step: step_name,
      input: sanitize_data(input_data),
      timestamp: DateTime.utc_now()
    })
    {:noreply, socket}
  end
  
  # Handle step completion events
  def handle_info({:step_completed, step_name, result, duration}, socket) do
    push(socket, "step_completed", %{
      step: step_name,
      result: sanitize_data(result),
      duration_ms: duration,
      timestamp: DateTime.utc_now()
    })
    {:noreply, socket}
  end
  
  # Handle step error events
  def handle_info({:step_error, step_name, error, context}, socket) do
    push(socket, "step_error", %{
      step: step_name,
      error: format_error(error),
      context: context,
      timestamp: DateTime.utc_now()
    })
    {:noreply, socket}
  end
  
  # Handle compensation events
  def handle_info({:step_compensation, step_name, compensation_result}, socket) do
    push(socket, "step_compensation", %{
      step: step_name,
      compensation: compensation_result,
      timestamp: DateTime.utc_now()
    })
    {:noreply, socket}
  end
  
  # Client requests step retry
  def handle_in("retry_step", %{"step" => step_name}, socket) do
    reactor_name = socket.assigns.reactor_name
    
    case retry_reactor_step(reactor_name, step_name) do
      {:ok, _} ->
        broadcast!(socket, "step_retried", %{
          step: step_name,
          retried_by: socket.assigns.user_id
        })
        {:reply, {:ok, %{status: "retried"}}, socket}
      
      {:error, reason} ->
        {:reply, {:error, %{reason: reason}}, socket}
    end
  end
  
  defp get_reactor_status(reactor_name) do
    %{
      reactor: reactor_name,
      current_step: nil,
      completed_steps: [],
      total_steps: 0,
      status: "idle",
      last_execution: nil
    }
  end
  
  defp sanitize_data(data) when is_map(data) do
    # Remove sensitive data before sending to client
    data
    |> Map.drop([:password, :secret, :token, :api_key])
    |> Enum.into(%{})
  end
  
  defp sanitize_data(data), do: data
  
  defp format_error(error) when is_exception(error) do
    %{
      type: error.__struct__,
      message: Exception.message(error)
    }
  end
  
  defp format_error(error), do: %{message: inspect(error)}
  
  defp retry_reactor_step(reactor_name, step_name) do
    Logger.info("Retrying reactor #{reactor_name} step #{step_name}")
    # Implementation would trigger actual step retry
    {:ok, %{retried: true}}
  end
end

defmodule CnsForge.TelemetryChannel do
  @moduledoc """
  Real-time telemetry streaming channel
  Streams performance metrics, throughput, and system health
  """
  
  use Phoenix.Channel
  require Logger
  
  def join("telemetry:" <> stream_type, _payload, socket) do
    Logger.info("Client joined telemetry channel: #{stream_type}")
    
    # Subscribe to telemetry streams
    Phoenix.PubSub.subscribe(CnsForge.PubSub, "telemetry:#{stream_type}")
    Phoenix.PubSub.subscribe(CnsForge.PubSub, "metrics:#{stream_type}")
    
    # Send current metrics snapshot
    current_metrics = get_current_metrics(stream_type)
    push(socket, "metrics_snapshot", current_metrics)
    
    # Start periodic metric updates
    schedule_metric_updates()
    
    socket = assign(socket, :stream_type, stream_type)
    {:ok, socket}
  end
  
  # Handle real-time metric updates
  def handle_info({:metric_update, metric_name, value, metadata}, socket) do
    push(socket, "metric", %{
      name: metric_name,
      value: value,
      metadata: metadata,
      timestamp: DateTime.utc_now()
    })
    {:noreply, socket}
  end
  
  # Handle aggregated metrics
  def handle_info({:aggregated_metrics, metrics}, socket) do
    push(socket, "aggregated_metrics", %{
      metrics: metrics,
      timestamp: DateTime.utc_now()
    })
    {:noreply, socket}
  end
  
  # Handle threshold alerts
  def handle_info({:threshold_alert, metric, threshold, current_value}, socket) do
    push(socket, "threshold_alert", %{
      metric: metric,
      threshold: threshold,
      current_value: current_value,
      severity: determine_severity(metric, threshold, current_value),
      timestamp: DateTime.utc_now()
    })
    {:noreply, socket}
  end
  
  # Periodic metric updates
  def handle_info(:metric_update_tick, socket) do
    stream_type = socket.assigns.stream_type
    
    # Collect latest metrics
    latest_metrics = collect_latest_metrics(stream_type)
    push(socket, "metrics_batch", latest_metrics)
    
    # Schedule next update
    schedule_metric_updates()
    {:noreply, socket}
  end
  
  # Client subscribes to specific metric
  def handle_in("subscribe_metric", %{"metric" => metric_name}, socket) do
    Phoenix.PubSub.subscribe(CnsForge.PubSub, "metric:#{metric_name}")
    {:reply, {:ok, %{subscribed: metric_name}}, socket}
  end
  
  # Client sets metric threshold
  def handle_in("set_threshold", %{"metric" => metric, "threshold" => threshold}, socket) do
    case set_metric_threshold(metric, threshold) do
      {:ok, _} ->
        {:reply, {:ok, %{threshold_set: true}}, socket}
      
      {:error, reason} ->
        {:reply, {:error, %{reason: reason}}, socket}
    end
  end
  
  defp get_current_metrics(stream_type) do
    case stream_type do
      "performance" ->
        %{
          cpu_usage: 45.2,
          memory_usage: 67.8,
          disk_io: 23.1,
          network_io: 15.6
        }
      
      "pipeline" ->
        %{
          throughput: 125.4,
          latency_p95: 45,
          error_rate: 0.12,
          active_pipelines: 8
        }
      
      "business" ->
        %{
          pipelines_completed: 1247,
          revenue_impact: 15750.00,
          sla_compliance: 99.7
        }
      
      _ ->
        %{}
    end
  end
  
  defp schedule_metric_updates do
    Process.send_after(self(), :metric_update_tick, 1000)  # Every second
  end
  
  defp collect_latest_metrics(stream_type) do
    # Collect metrics based on stream type
    base_metrics = get_current_metrics(stream_type)
    
    # Add some variance to simulate real metrics
    Enum.map(base_metrics, fn {key, value} ->
      variance = (value * 0.1 * (:rand.uniform() - 0.5))
      {key, value + variance}
    end)
    |> Enum.into(%{})
  end
  
  defp determine_severity(metric, threshold, current_value) do
    ratio = current_value / threshold
    
    cond do
      ratio >= 2.0 -> "critical"
      ratio >= 1.5 -> "high"
      ratio >= 1.2 -> "medium"
      true -> "low"
    end
  end
  
  defp set_metric_threshold(metric, threshold) do
    Logger.info("Setting threshold for #{metric}: #{threshold}")
    # Implementation would store threshold configuration
    {:ok, %{threshold_set: true}}
  end
end

defmodule CnsForge.PipelineEventProcessor do
  @moduledoc """
  Processes pipeline events and broadcasts them to appropriate channels
  Integrates with the existing Ash Reactor infrastructure
  """
  
  use GenServer
  require Logger
  
  def start_link(opts) do
    GenServer.start_link(__MODULE__, opts, name: __MODULE__)
  end
  
  def init(_opts) do
    # Subscribe to pipeline events from existing infrastructure
    setup_event_subscriptions()
    
    {:ok, %{}}
  end
  
  # Public API for broadcasting events
  def broadcast_stage_transition(pipeline_id, stage, from_status, to_status, data \\\\ %{}) do
    Phoenix.PubSub.broadcast(CnsForge.PubSub, "pipeline_events:#{pipeline_id}", 
      {:stage_transition, stage, from_status, to_status, data})
  end
  
  def broadcast_stage_completed(pipeline_id, stage, result, metadata \\\\ %{}) do
    Phoenix.PubSub.broadcast(CnsForge.PubSub, "pipeline_events:#{pipeline_id}", 
      {:stage_completed, stage, result, metadata})
  end
  
  def broadcast_pipeline_failure(pipeline_id, stage, error, context \\\\ %{}) do
    Phoenix.PubSub.broadcast(CnsForge.PubSub, "pipeline_events:#{pipeline_id}", 
      {:pipeline_failure, stage, error, context})
  end
  
  defp setup_event_subscriptions do
    # Subscribe to existing pipeline events
    Phoenix.PubSub.subscribe(CnsForge.PubSub, "pipeline:events")
    Phoenix.PubSub.subscribe(CnsForge.PubSub, "reactor:steps")
    Phoenix.PubSub.subscribe(CnsForge.PubSub, "telemetry:performance")
  end
  
  # Handle events from existing pipeline infrastructure
  def handle_info({:pipeline_stage, stage, status, data}, state) do
    # Extract pipeline ID from data or generate one
    pipeline_id = Map.get(data, :pipeline_id, "default")
    
    # Broadcast to appropriate channels
    broadcast_stage_transition(pipeline_id, stage, "running", status, data)
    
    {:noreply, state}
  end
  
  def handle_info({:reactor_step, step_name, result}, state) do
    # Broadcast reactor step events
    Phoenix.PubSub.broadcast(CnsForge.PubSub, "reactor_steps:default", 
      {:step_completed, step_name, result, System.monotonic_time(:millisecond)})
    
    {:noreply, state}
  end
end

defmodule CnsForge.ReactorStepNotifier do
  @moduledoc """
  Integrates with Ash Reactor to provide real-time step notifications
  """
  
  use GenServer
  require Logger
  
  def start_link(opts) do
    GenServer.start_link(__MODULE__, opts, name: __MODULE__)
  end
  
  def init(_opts) do
    # Setup telemetry handlers for reactor steps
    setup_reactor_telemetry()
    
    {:ok, %{}}
  end
  
  # Notify step start
  def notify_step_started(reactor_name, step_name, input_data) do
    Phoenix.PubSub.broadcast(CnsForge.PubSub, "reactor_steps:#{reactor_name}", 
      {:step_started, step_name, input_data})
  end
  
  # Notify step completion
  def notify_step_completed(reactor_name, step_name, result, duration) do
    Phoenix.PubSub.broadcast(CnsForge.PubSub, "reactor_steps:#{reactor_name}", 
      {:step_completed, step_name, result, duration})
  end
  
  # Notify step error
  def notify_step_error(reactor_name, step_name, error, context) do
    Phoenix.PubSub.broadcast(CnsForge.PubSub, "reactor_steps:#{reactor_name}", 
      {:step_error, step_name, error, context})
    
    # Also send to error tracking
    Phoenix.PubSub.broadcast(CnsForge.PubSub, "reactor_errors:#{reactor_name}", 
      {:step_error, step_name, error, context})
  end
  
  defp setup_reactor_telemetry do
    # Attach telemetry handlers for reactor events
    events = [
      [:reactor, :step, :start],
      [:reactor, :step, :stop],
      [:reactor, :step, :exception]
    ]
    
    Enum.each(events, fn event ->
      :telemetry.attach(
        "reactor-#{Enum.join(event, "-")}",
        event,
        &handle_reactor_telemetry/4,
        %{}
      )
    end)
  end
  
  defp handle_reactor_telemetry([:reactor, :step, :start], measurements, metadata, _config) do
    reactor_name = Map.get(metadata, :reactor, "unknown")
    step_name = Map.get(metadata, :step, "unknown")
    
    notify_step_started(reactor_name, step_name, metadata)
  end
  
  defp handle_reactor_telemetry([:reactor, :step, :stop], measurements, metadata, _config) do
    reactor_name = Map.get(metadata, :reactor, "unknown")
    step_name = Map.get(metadata, :step, "unknown")
    duration = Map.get(measurements, :duration, 0)
    
    notify_step_completed(reactor_name, step_name, metadata, duration)
  end
  
  defp handle_reactor_telemetry([:reactor, :step, :exception], measurements, metadata, _config) do
    reactor_name = Map.get(metadata, :reactor, "unknown")
    step_name = Map.get(metadata, :step, "unknown")
    error = Map.get(metadata, :error, "unknown error")
    
    notify_step_error(reactor_name, step_name, error, metadata)
  end
end

defmodule CnsForge.TelemetryStreamer do
  @moduledoc """
  Streams telemetry data to real-time channels
  """
  
  use GenServer
  require Logger
  
  def start_link(opts) do
    GenServer.start_link(__MODULE__, opts, name: __MODULE__)
  end
  
  def init(_opts) do
    # Setup telemetry collection
    setup_telemetry_handlers()
    
    # Start periodic metric collection
    schedule_metric_collection()
    
    {:ok, %{metric_buffer: %{}}}
  end
  
  # Emit telemetry metric
  def emit_metric(stream_type, metric_name, value, metadata \\\\ %{}) do
    Phoenix.PubSub.broadcast(CnsForge.PubSub, "telemetry:#{stream_type}", 
      {:metric_update, metric_name, value, metadata})
  end
  
  # Handle periodic metric collection
  def handle_info(:collect_metrics, state) do
    # Collect various metrics
    performance_metrics = collect_performance_metrics()
    pipeline_metrics = collect_pipeline_metrics()
    business_metrics = collect_business_metrics()
    
    # Broadcast to appropriate streams
    emit_metric("performance", "system_health", performance_metrics)
    emit_metric("pipeline", "pipeline_status", pipeline_metrics)
    emit_metric("business", "business_kpis", business_metrics)
    
    # Schedule next collection
    schedule_metric_collection()
    
    {:noreply, state}
  end
  
  defp setup_telemetry_handlers do
    # Setup handlers for various telemetry events
    telemetry_events = [
      [:vm, :memory],
      [:phoenix, :endpoint, :start],
      [:phoenix, :endpoint, :stop]
    ]
    
    Enum.each(telemetry_events, fn event ->
      :telemetry.attach(
        "telemetry-#{Enum.join(event, "-")}",
        event,
        &handle_telemetry_event/4,
        %{}
      )
    end)
  end
  
  defp handle_telemetry_event(event, measurements, metadata, _config) do
    # Process telemetry event and emit to appropriate stream
    stream_type = determine_stream_type(event)
    metric_name = Enum.join(event, ".")
    
    emit_metric(stream_type, metric_name, measurements, metadata)
  end
  
  defp determine_stream_type(event) do
    case event do
      [:vm, _] -> "performance"
      [:phoenix, _] -> "performance" 
      [:reactor, _] -> "pipeline"
      _ -> "general"
    end
  end
  
  defp schedule_metric_collection do
    Process.send_after(self(), :collect_metrics, 5000)  # Every 5 seconds
  end
  
  defp collect_performance_metrics do
    %{
      memory_usage: :erlang.memory(:total),
      process_count: :erlang.system_info(:process_count),
      cpu_usage: get_cpu_usage(),
      uptime: :erlang.statistics(:wall_clock) |> elem(0)
    }
  end
  
  defp collect_pipeline_metrics do
    %{
      active_pipelines: count_active_pipelines(),
      completed_today: count_completed_pipelines_today(),
      error_rate: calculate_error_rate(),
      avg_duration: calculate_avg_duration()
    }
  end
  
  defp collect_business_metrics do
    %{
      pipelines_processed: get_total_pipelines_processed(),
      uptime_percentage: calculate_uptime_percentage(),
      sla_compliance: calculate_sla_compliance()
    }
  end
  
  # Simplified metric calculations (replace with real implementations)
  defp get_cpu_usage, do: :rand.uniform(100)
  defp count_active_pipelines, do: :rand.uniform(10)
  defp count_completed_pipelines_today, do: :rand.uniform(100)
  defp calculate_error_rate, do: :rand.uniform() * 5
  defp calculate_avg_duration, do: :rand.uniform(1000)
  defp get_total_pipelines_processed, do: :rand.uniform(10000)
  defp calculate_uptime_percentage, do: 99.0 + :rand.uniform()
  defp calculate_sla_compliance, do: 98.0 + :rand.uniform() * 2
end

# Supporting modules for supervision
defmodule CnsForge.PipelineChannelSupervisor do
  use Supervisor
  
  def start_link(opts) do
    Supervisor.start_link(__MODULE__, opts, name: __MODULE__)
  end
  
  def init(_opts) do
    children = []
    Supervisor.init(children, strategy: :one_for_one)
  end
end

defmodule CnsForge.ReactorStepChannelSupervisor do
  use Supervisor
  
  def start_link(opts) do
    Supervisor.start_link(__MODULE__, opts, name: __MODULE__)
  end
  
  def init(_opts) do
    children = []
    Supervisor.init(children, strategy: :one_for_one)
  end
end

defmodule CnsForge.TelemetryChannelSupervisor do
  use Supervisor
  
  def start_link(opts) do
    Supervisor.start_link(__MODULE__, opts, name: __MODULE__)
  end
  
  def init(_opts) do
    children = []
    Supervisor.init(children, strategy: :one_for_one)
  end
end