name: BitActor UHFT CI/CD Pipeline

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]
  workflow_dispatch:
    inputs:
      environment:
        description: 'Deployment environment'
        required: true
        default: 'staging'
        type: choice
        options:
        - staging
        - production
        
env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}
  
jobs:
  # Performance regression detection
  benchmark:
    name: Performance Benchmarks
    runs-on: [self-hosted, metal, uhft]  # Bare metal runner for consistent results
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Setup Erlang/OTP
      uses: erlef/setup-beam@v1
      with:
        otp-version: '28.0'
        rebar3-version: '3.22.0'
        
    - name: Install dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y \
          linux-tools-common \
          linux-tools-generic \
          linux-tools-`uname -r` \
          libnuma-dev
        
    - name: Cache dependencies
      uses: actions/cache@v3
      with:
        path: |
          _build
          ~/.cache/rebar3
        key: ${{ runner.os }}-mix-${{ hashFiles('**/rebar.lock') }}
        
    - name: Compile with optimizations
      run: |
        export CFLAGS="-O3 -march=native -mtune=native"
        make clean
        make compile
        
    - name: Run latency benchmarks
      run: |
        # Disable CPU frequency scaling for consistent results
        echo performance | sudo tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor
        
        # Pin to specific CPU cores
        taskset -c 0-7 make benchmark > benchmark_results.txt
        
    - name: Validate performance targets
      run: |
        # Parse results and check against targets
        python3 scripts/validate_benchmarks.py \
          --results benchmark_results.txt \
          --targets config/performance_targets.yaml \
          --output benchmark_report.json
          
    - name: Upload benchmark results
      uses: actions/upload-artifact@v3
      with:
        name: benchmark-results
        path: |
          benchmark_results.txt
          benchmark_report.json
          
    - name: Comment PR with results
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const report = JSON.parse(fs.readFileSync('benchmark_report.json', 'utf8'));
          
          const comment = `## 🚀 Performance Benchmark Results
          
          | Metric | Target | Actual | Status |
          |--------|--------|--------|--------|
          | Market Data P99 | <500ns | ${report.market_data_p99}ns | ${report.market_data_status} |
          | Order Engine P99 | <1μs | ${report.order_engine_p99}ns | ${report.order_engine_status} |
          | Message Throughput | >1M/sec | ${report.throughput}/sec | ${report.throughput_status} |
          
          ${report.pass ? '✅ All performance targets met!' : '❌ Performance regression detected!'}
          `;
          
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: comment
          });
          
  # Security scanning
  security:
    name: Security Analysis
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Run Trivy vulnerability scanner
      uses: aquasecurity/trivy-action@master
      with:
        scan-type: 'fs'
        scan-ref: '.'
        format: 'sarif'
        output: 'trivy-results.sarif'
        severity: 'CRITICAL,HIGH'
        
    - name: Upload Trivy scan results
      uses: github/codeql-action/upload-sarif@v2
      with:
        sarif_file: 'trivy-results.sarif'
        
    - name: SAST with Semgrep
      uses: returntocorp/semgrep-action@v1
      with:
        config: >-
          p/security-audit
          p/erlang
          
  # Build multi-arch containers
  build:
    name: Build Container Images
    runs-on: ubuntu-latest
    needs: [benchmark, security]
    strategy:
      matrix:
        component: [market-data, order-engine, risk-engine, alpha-calculator, execution-gateway]
        
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up QEMU
      uses: docker/setup-qemu-action@v3
      
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
      
    - name: Log in to Container Registry
      uses: docker/login-action@v3
      with:
        registry: ${{ env.REGISTRY }}
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}
        
    - name: Extract metadata
      id: meta
      uses: docker/metadata-action@v5
      with:
        images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}/${{ matrix.component }}
        tags: |
          type=ref,event=branch
          type=ref,event=pr
          type=semver,pattern={{version}}
          type=semver,pattern={{major}}.{{minor}}
          type=sha
          
    - name: Build and push image
      uses: docker/build-push-action@v5
      with:
        context: .
        file: docker/Dockerfile.${{ matrix.component }}
        platforms: linux/amd64,linux/arm64
        push: true
        tags: ${{ steps.meta.outputs.tags }}
        labels: ${{ steps.meta.outputs.labels }}
        cache-from: type=gha
        cache-to: type=gha,mode=max
        build-args: |
          BUILD_DATE=${{ github.event.repository.updated_at }}
          VCS_REF=${{ github.sha }}
          VERSION=${{ steps.meta.outputs.version }}
          
  # Integration tests in Kubernetes
  integration-test:
    name: Integration Tests
    runs-on: ubuntu-latest
    needs: build
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Create kind cluster
      uses: helm/kind-action@v1.8.0
      with:
        cluster_name: bitactor-test
        config: test/kind-config.yaml
        
    - name: Install Multus CNI
      run: |
        kubectl apply -f https://raw.githubusercontent.com/k8snetworkplumbingwg/multus-cni/master/deployments/multus-daemonset.yml
        
    - name: Deploy BitActor
      run: |
        helm dependency update infrastructure/kubernetes/helm/bitactor
        helm install bitactor-test infrastructure/kubernetes/helm/bitactor \
          --namespace bitactor-test \
          --create-namespace \
          --values test/values-integration.yaml \
          --wait \
          --timeout 10m
          
    - name: Run integration tests
      run: |
        kubectl apply -f test/integration-test-job.yaml
        kubectl wait --for=condition=complete job/integration-test -n bitactor-test --timeout=10m
        
    - name: Collect test results
      if: always()
      run: |
        kubectl logs -n bitactor-test job/integration-test > integration-test-results.log
        kubectl get pods -n bitactor-test -o wide > pod-status.log
        
    - name: Upload test results
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: integration-test-results
        path: |
          integration-test-results.log
          pod-status.log
          
  # Chaos testing
  chaos-test:
    name: Chaos Engineering Tests
    runs-on: ubuntu-latest
    needs: integration-test
    if: github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Install Litmus Chaos
      run: |
        kubectl apply -f https://litmuschaos.github.io/litmus/litmus-operator-v3.0.0.yaml
        kubectl apply -f https://hub.litmuschaos.io/api/chaos/3.0.0?file=charts/generic/experiments.yaml
        
    - name: Run chaos experiments
      run: |
        kubectl apply -f test/chaos/network-latency.yaml
        kubectl apply -f test/chaos/pod-failure.yaml
        kubectl apply -f test/chaos/cpu-stress.yaml
        
    - name: Validate resilience
      run: |
        sleep 300  # Let chaos run
        python3 scripts/validate_resilience.py \
          --prometheus http://prometheus:9090 \
          --duration 5m \
          --slo-target 0.999
          
  # Progressive deployment
  deploy:
    name: Deploy to Environment
    runs-on: ubuntu-latest
    needs: [integration-test]
    if: github.ref == 'refs/heads/main' || github.event_name == 'workflow_dispatch'
    environment:
      name: ${{ github.event.inputs.environment || 'staging' }}
      url: https://${{ github.event.inputs.environment || 'staging' }}.bitactor.io
      
    steps:
    - uses: actions/checkout@v4
    
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/bitactor-deploy
        aws-region: us-east-1
        
    - name: Update ArgoCD Application
      run: |
        # Update image tags in ArgoCD
        yq eval -i '.spec.source.helm.parameters[] |= 
          (select(.name == "image.tag").value = "${{ github.sha }}")' \
          infrastructure/kubernetes/gitops/argocd/bitactor-app.yaml
          
        # Commit changes
        git config user.name github-actions
        git config user.email github-actions@github.com
        git add infrastructure/kubernetes/gitops/argocd/bitactor-app.yaml
        git commit -m "Deploy ${{ github.sha }} to ${{ github.event.inputs.environment || 'staging' }}"
        git push
        
    - name: Wait for ArgoCD sync
      run: |
        argocd app wait bitactor-${{ github.event.inputs.environment || 'staging' }} \
          --sync \
          --health \
          --timeout 600
          
    - name: Run smoke tests
      run: |
        ./scripts/smoke_test.sh https://${{ github.event.inputs.environment || 'staging' }}.bitactor.io
        
    - name: Notify deployment
      uses: 8398a7/action-slack@v3
      with:
        status: ${{ job.status }}
        text: |
          Deployment to ${{ github.event.inputs.environment || 'staging' }} ${{ job.status }}
          Commit: ${{ github.sha }}
          Author: ${{ github.actor }}
      env:
        SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK }}
        
  # Performance monitoring post-deployment
  monitor:
    name: Post-Deployment Monitoring
    runs-on: ubuntu-latest
    needs: deploy
    if: success()
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Run performance validation
      run: |
        python3 scripts/post_deploy_validation.py \
          --environment ${{ github.event.inputs.environment || 'staging' }} \
          --duration 15m \
          --prometheus https://prometheus.${{ github.event.inputs.environment || 'staging' }}.bitactor.io \
          --grafana https://grafana.${{ github.event.inputs.environment || 'staging' }}.bitactor.io
          
    - name: Generate performance report
      run: |
        python3 scripts/generate_performance_report.py \
          --environment ${{ github.event.inputs.environment || 'staging' }} \
          --output performance_report.html
          
    - name: Upload performance report
      uses: actions/upload-artifact@v3
      with:
        name: performance-report-${{ github.event.inputs.environment || 'staging' }}
        path: performance_report.html