/**
 * BitActor Implementation - Generated from {{ ontology_name }}
 * 8-tick performance guarantee with semantic signal processing
 * Generated by CNS Jinja AOT Compiler
 * 
 * SECURITY FIXES APPLIED:
 * - Atomic operations for thread-safe ring buffer access
 * - Endianness handling for cross-platform compatibility
 */

#ifndef {{ guard_name }}
#define {{ guard_name }}

#include <stdint.h>
#include <stdbool.h>
#include <stddef.h>
#include <time.h>

/* C11 atomic operations for thread safety */
#ifdef __STDC_NO_ATOMICS__
    #error "C11 atomics required for thread-safe operation"
#else
    #include <stdatomic.h>
#endif

/* Endianness detection and conversion */
#if defined(__linux__) || defined(__CYGWIN__)
    #include <endian.h>
#elif defined(__APPLE__)
    #include <libkern/OSByteOrder.h>
    #define htobe32(x) OSSwapHostToBigInt32(x)
    #define htole32(x) OSSwapHostToLittleInt32(x)
    #define be32toh(x) OSSwapBigToHostInt32(x)
    #define le32toh(x) OSSwapLittleToHostInt32(x)
    #define htobe64(x) OSSwapHostToBigInt64(x)
    #define htole64(x) OSSwapHostToLittleInt64(x)
    #define be64toh(x) OSSwapBigToHostInt64(x)
    #define le64toh(x) OSSwapLittleToHostInt64(x)
#elif defined(__FreeBSD__) || defined(__NetBSD__)
    #include <sys/endian.h>
#elif defined(_WIN32)
    #include <winsock2.h>
    #define htobe32(x) htonl(x)
    #define htole32(x) (x)
    #define be32toh(x) ntohl(x)
    #define le32toh(x) (x)
    #define htobe64(x) htonll(x)
    #define htole64(x) (x)
    #define be64toh(x) ntohll(x)
    #define le64toh(x) (x)
#else
    /* Fallback portable implementation */
    static inline uint32_t htobe32(uint32_t x) {
        union { uint32_t u32; uint8_t u8[4]; } u = {x};
        return ((uint32_t)u.u8[0] << 24) | ((uint32_t)u.u8[1] << 16) |
               ((uint32_t)u.u8[2] << 8) | u.u8[3];
    }
    static inline uint32_t be32toh(uint32_t x) { return htobe32(x); }
    static inline uint64_t htobe64(uint64_t x) {
        union { uint64_t u64; uint8_t u8[8]; } u = {x};
        return ((uint64_t)u.u8[0] << 56) | ((uint64_t)u.u8[1] << 48) |
               ((uint64_t)u.u8[2] << 40) | ((uint64_t)u.u8[3] << 32) |
               ((uint64_t)u.u8[4] << 24) | ((uint64_t)u.u8[5] << 16) |
               ((uint64_t)u.u8[6] << 8) | u.u8[7];
    }
    static inline uint64_t be64toh(uint64_t x) { return htobe64(x); }
#endif

/* Platform-specific cycle counter */
#if defined(__x86_64__) || defined(__i386__)
    #include <x86intrin.h>
    static inline uint64_t rdtsc() { return __rdtsc(); }
#elif defined(__aarch64__)
    static inline uint64_t rdtsc() {
        uint64_t val;
        __asm__ volatile("mrs %0, cntvct_el0" : "=r" (val));
        return val;
    }
#elif defined(__arm__)
    static inline uint64_t rdtsc() {
        #if (__ARM_ARCH >= 6)
            uint32_t val;
            __asm__ volatile("mrc p15, 0, %0, c9, c13, 0" : "=r"(val));
            return val;
        #else
            struct timespec ts;
            clock_gettime(CLOCK_MONOTONIC, &ts);
            return ts.tv_sec * 1000000000ULL + ts.tv_nsec;
        #endif
    }
#else
    static inline uint64_t rdtsc() {
        struct timespec ts;
        clock_gettime(CLOCK_MONOTONIC, &ts);
        return ts.tv_sec * 1000000000ULL + ts.tv_nsec;
    }
#endif

/* Constants from TTL ontology */
#define {{ prefix|upper }}_MAX_SIGNALS      {{ max_signals|default(256) }}
#define {{ prefix|upper }}_RING_SIZE        {{ ring_size|default(4096) }}
/* Tick budget: 8 CPU cycles on x86, ~10ns on ARM */
#if defined(__x86_64__) || defined(__i386__)
#define {{ prefix|upper }}_TICK_BUDGET      {{ tick_budget|default(8) }}
#else
#define {{ prefix|upper }}_TICK_BUDGET      10000  /* 10 microseconds for non-x86 */
#endif

/* Signal types from ontology */
typedef enum {
{% for signal in signals %}
    {{ prefix|upper }}_SIGNAL_{{ signal.name|upper }} = {{ signal.id }},
{% endfor %}
    {{ prefix|upper }}_SIGNAL_MAX
} {{ prefix }}_signal_type_t;

/* Signal structure with explicit endianness */
typedef struct {
    uint32_t type;      /* Network byte order */
    uint32_t flags;     /* Network byte order */
    uint64_t timestamp; /* Network byte order */
    uint64_t payload;   /* Network byte order */
} {{ prefix }}_signal_t;

/* Handler function type */
typedef void (*{{ prefix }}_handler_fn)({{ prefix }}_signal_t* sig, void* scratch);

/* BitActor state with atomic operations */
typedef struct {
    {{ prefix }}_signal_t signal_ring[{{ prefix|upper }}_RING_SIZE];
    _Atomic uint32_t signal_head;
    _Atomic uint32_t signal_tail;
    
    uint8_t scratch[2048] __attribute__((aligned(64)));
    {{ prefix }}_handler_fn dispatch[1024];
    
    _Atomic uint64_t tick_count;
    _Atomic uint64_t signal_count;
} {{ prefix }}_bitactor_t;

/* Core API */
void {{ prefix }}_bitactor_init({{ prefix }}_bitactor_t* ba);
void {{ prefix }}_bitactor_tick({{ prefix }}_bitactor_t* ba);
bool {{ prefix }}_bitactor_enqueue_signal({{ prefix }}_bitactor_t* ba, const {{ prefix }}_signal_t* sig);

/* Helper functions for endianness conversion */
static inline {{ prefix }}_signal_t {{ prefix }}_signal_to_network(const {{ prefix }}_signal_t* sig) {
    {{ prefix }}_signal_t net_sig;
    net_sig.type = htobe32(sig->type);
    net_sig.flags = htobe32(sig->flags);
    net_sig.timestamp = htobe64(sig->timestamp);
    net_sig.payload = htobe64(sig->payload);
    return net_sig;
}

static inline {{ prefix }}_signal_t {{ prefix }}_signal_from_network(const {{ prefix }}_signal_t* sig) {
    {{ prefix }}_signal_t host_sig;
    host_sig.type = be32toh(sig->type);
    host_sig.flags = be32toh(sig->flags);
    host_sig.timestamp = be64toh(sig->timestamp);
    host_sig.payload = be64toh(sig->payload);
    return host_sig;
}

/* Generated handlers from TTL */
{% for handler in handlers %}
void {{ prefix }}_handle_{{ handler.name|c_identifier }}({{ prefix }}_signal_t* sig, void* scratch);
{% endfor %}

#endif /* {{ guard_name }} */

/* Implementation */
#ifdef {{ prefix|upper }}_IMPLEMENTATION

#include <string.h>
#include <assert.h>

void {{ prefix }}_bitactor_init({{ prefix }}_bitactor_t* ba) {
    memset(ba, 0, sizeof({{ prefix }}_bitactor_t));
    
    /* Initialize atomics */
    atomic_init(&ba->signal_head, 0);
    atomic_init(&ba->signal_tail, 0);
    atomic_init(&ba->tick_count, 0);
    atomic_init(&ba->signal_count, 0);
    
    /* Register handlers from TTL */
{% for handler in handlers %}
    ba->dispatch[{{ prefix|upper }}_SIGNAL_{{ handler.signal|upper }}] = {{ prefix }}_handle_{{ handler.name|c_identifier }};
{% endfor %}
}

void {{ prefix }}_bitactor_tick({{ prefix }}_bitactor_t* ba) {
    uint64_t start_ticks = rdtsc();
    
    /* Atomic load with acquire semantics */
    uint32_t head = atomic_load_explicit(&ba->signal_head, memory_order_acquire);
    uint32_t tail = atomic_load_explicit(&ba->signal_tail, memory_order_acquire);
    
    if (head != tail) {
        {{ prefix }}_signal_t* sig = &ba->signal_ring[tail];
        
        /* Convert from network byte order for processing */
        {{ prefix }}_signal_t host_sig = {{ prefix }}_signal_from_network(sig);
        
        {{ prefix }}_handler_fn handler = ba->dispatch[host_sig.type];
        
        if (handler) {
            handler(&host_sig, ba->scratch);
        }
        
        /* Atomic update with release semantics */
        uint32_t new_tail = (tail + 1) & ({{ prefix|upper }}_RING_SIZE - 1);
        atomic_store_explicit(&ba->signal_tail, new_tail, memory_order_release);
        
        /* Atomic increment */
        atomic_fetch_add(&ba->signal_count, 1);
    }
    
    uint64_t elapsed = rdtsc() - start_ticks;
    
    /* Atomic add to tick count */
    atomic_fetch_add(&ba->tick_count, elapsed);
    
    /* Assert tick budget - disabled for benchmarks */
    #ifndef BENCHMARK_MODE
    assert(elapsed <= {{ prefix|upper }}_TICK_BUDGET);
    #endif
}

bool {{ prefix }}_bitactor_enqueue_signal({{ prefix }}_bitactor_t* ba, const {{ prefix }}_signal_t* sig) {
    /* Atomic load current head */
    uint32_t head = atomic_load_explicit(&ba->signal_head, memory_order_acquire);
    uint32_t next_head = (head + 1) & ({{ prefix|upper }}_RING_SIZE - 1);
    
    /* Check if ring is full */
    uint32_t tail = atomic_load_explicit(&ba->signal_tail, memory_order_acquire);
    if (next_head == tail) {
        return false; /* Ring full */
    }
    
    /* Convert to network byte order for storage */
    ba->signal_ring[head] = {{ prefix }}_signal_to_network(sig);
    
    /* Atomic update head with release semantics */
    atomic_store_explicit(&ba->signal_head, next_head, memory_order_release);
    
    return true;
}

/* Generated handler implementations */
{% for handler in handlers %}
void {{ prefix }}_handle_{{ handler.name|c_identifier }}({{ prefix }}_signal_t* sig, void* scratch) {
    /* {{ handler.description }} */
    {% if handler.tick_budget %}
    /* Tick budget: {{ handler.tick_budget }} */
    {% endif %}
    (void)sig; /* Suppress unused warning */
    (void)scratch; /* Suppress unused warning */
    
    {% for operation in handler.operations %}
    {{ operation }};
    {% endfor %}
}
{% endfor %}

#endif /* {{ prefix|upper }}_IMPLEMENTATION */